[
["index.html", "Introduction to Data Science Chapter 1 Introduction to Data Science 1.1 Chapter learning objectives 1.2 Jupyter notebooks 1.3 Loading a spreadsheet-like dataset", " Introduction to Data Science Tiffany-Anne Timbers Trevor Campbell Melissa Lee 2020-08-14 Chapter 1 Introduction to Data Science This is an open source textbook aimed at introducing undergraduate students to data science. It was originally written for the University of British Columbia’s DSCI 100 - Introduction to Data Science course. In this book, we define data science as the study and development of reproducible, auditable processes to obtain value (i.e., insight) from data. The book is structured so that learners spend the first four chapters learning how to use the R programming language and Jupyter notebooks to load, wrangle/clean, and visualize data, while answering descriptive and exploratory data analysis questions. The remaining chapters illustrate how to solve four common problems in data science, which are useful for answering predictive and inferential data analysis questions: Predicting a class/category for a new observation/measurement (e.g., cancerous or benign tumour) Predicting a value for a new observation/measurement (e.g., 10 km race time for 20 year old females with a BMI of 25). Finding previously unknown/unlabelled subgroups in your data (e.g., products commonly bought together on Amazon) Estimating an average or a proportion from a representative sample (group of people or units) and using that estimate to generalize to the broader population (e.g., the proportion of undergraduate students that own an iphone) For each of these problems, we map them to the type of data analysis question being asked and discuss what kinds of data are needed to answer such questions. More advanced (e.g., causal or mechanistic) data analysis questions are beyond the scope of this text. Types of data analysis questions Question type Description Example Descriptive A question which asks about summarized characteristics of a data set without interpretation (i.e., report a fact). How many people live in each US state? Exploratory A question asks if there are patterns, trends, or relationships within a single data set. Often used to propose hypotheses for future study. Does politcal party voting change with indicators of wealth in a set of data collected from groups of individuals from several regions in the United States? Inferential A question that looks for patterns, trends, or relationships in a single data set and also asks for quantification of how applicable these findings are to the wider population. Does politcal party voting change with indicators of wealth in the United States? Predictive A question that asks about predicting measurements or labels for individuals (people or things). The focus is on what things predict some outcome, but not what causes the outcome. What political party will someone vote for in the next US election? Causal A question that asks about whether changing one factor will lead to a change in another factor, on average, in the wider population. Does wealth lead to voting for a certain political party candidate in the US Presidential election? Mechanistic A question that asks about the underlying mechanism of the observed patterns, trends, or relationship (i.e., how does it happen?) How does wealth lead to voting for a certain political party candidate in the US Presidential election? Source: What is the question? by Jeffery T. Leek, Roger D. Peng &amp; The Art of Data Science by Roger Peng &amp; Elizabeth Matsui 1.1 Chapter learning objectives By the end of the chapter, students will be able to: use a Jupyter notebook to execute provided R code edit code and markdown cells in a Jupyter notebook create new code and markdown cells in a Jupyter notebook load the tidyverse library into R create new variables and objects in R using the assignment symbol use the help and documentation tools in R match the names of the following functions from the tidyverse library to their documentation descriptions: read_csv select mutate filter ggplot aes 1.2 Jupyter notebooks Jupyter notebooks are documents that contain a mix of computer code (and its output) and formattable text. Given that they are able to combine these two in a single document—code is not separate from the output or written report—notebooks are one of the leading tools to create reproducible data analyses. A reproducible data analysis is one where you can reliably and easily recreate the same results when analyzing the same data. Although this sounds like something that should always be true of any data analysis, in reality this is not often the case; one needs to make a conscious effort to perform data analysis in a reproducible manner. The name Jupyter came from combining the names of the three programming language that it was initially targeted for (Julia, Python, and R), and now many other languages can be used with Jupyter notebooks. A notebook looks like this: We have included a short demo video here to help you get started and to introduce you to R and Jupyter. However, the best way to learn how to write and run code and formattable text in a Jupyter notebook is to do it yourself! Here is a worksheet that provides a step-by-step guide through the basics. 1.3 Loading a spreadsheet-like dataset Often, the first thing we need to do in data analysis is to load a dataset into R. When we bring spreadsheet-like (think Microsoft Excel tables) data, generally shaped like a rectangle, into R it is represented as what we call a data frame object. It is very similar to a spreadsheet where the rows are the collected observations and the columns are the variables. The first kind of data we will learn how to load into R (as a data frame) is the spreadsheet-like comma-separated values format (.csv for short). These files have names ending in .csv, and can be opened open and saved from common spreadsheet programs like Microsoft Excel and Google Sheets. For example, a .csv file named state_property_vote.csv is included with the code for this book. This file— originally from Data USA—has US state-level property, income, population and voting data from 2015 and 2016. If we were to open this data in a plain text editor, we would see each row on its own line, and each entry in the table separated by a comma: "],
["making-r-source-code-chunk-interactive-inside-html-document.html", "Chapter 2 Making R source code chunk interactive inside HTML document 2.1 Assigning value to a data frame 2.2 Creating subsets of data frames with select &amp; filter 2.3 Exploring data with visualizations", " Chapter 2 Making R source code chunk interactive inside HTML document Here we add a button to activate interactivity in chunks. By clicking on this button, it will make the chunks interactive and it will launch an R kernel to execute some code from these interactive chunks. state,med_income,med_prop_val,population,mean_commute_minutes,party AK,64222,197300,733375,10.46830207,Republican AL,36924,94800,4830620,25.30990746,Republican AR,35833,83300,2958208,22.40108933,Republican AZ,44748,128700,6641928,20.58786,Republican CA,53075,252100,38421464,23.38085172,Democrat CO,48098,198900,5278906,19.50792188,Democrat CT,69228,246450,3593222,24.349675,Democrat DC,70848,475800,647484,28.2534,Democrat DE,54976,228500,926454,24.45553333,Democrat To load this data into R, and then to do anything else with it afterwards, we will need to use something called a function. A function is a special word in R that takes in instructions (we call these arguments) and does something. The function we will use to read a .csv file into R is called read_csv. In its most basic use-case, read_csv expects that the data file: has column names (or headers), uses a comma (,) to separate the columns, and does not have row names. Below you’ll see the code used to load the data into R using the read_csv function. But there is one extra step we need to do first. Since read_csv is not included in the base installation of R, to be able to use it we have to load it from somewhere else: a collection of useful functions known as a library. The read_csv function in particular is in the tidyverse library (more on this later), which we load using the library function. Next, we call the read_csv function and pass it a single argument: the name of the file, \"state_property_vote.csv\". We have to put quotes around filenames and other letters and words that we use in our code to distinguish it from the special words that make up R programming language. This is the only argument we need to provide for this file, because our file satifies everthing else the read_csv function expects in the default use-case (which we just discussed). Later in the course, we’ll learn more about how to deal with more complicated files where the default arguments are not appropriate. For example, files that use spaces or tabs to separate the columns, or with no column names. clicking the below button will make this book interactive and that could take some times to strat. Be patient… Activate us_data &lt;- read_csv(&quot;https://raw.githubusercontent.com/UBC-DSCI/introduction-to-datascience/master/state_property_vote.csv&quot;) ## Parsed with column specification: ## cols( ## state = col_character(), ## med_income = col_double(), ## med_prop_val = col_double(), ## population = col_double(), ## mean_commute_minutes = col_double(), ## party = col_character() ## ) write_csv(us_data, &quot;state_property_vote.csv&quot;) library(tidyverse) read_csv(&quot;state_property_vote.csv&quot;) ## Parsed with column specification: ## cols( ## state = col_character(), ## med_income = col_double(), ## med_prop_val = col_double(), ## population = col_double(), ## mean_commute_minutes = col_double(), ## party = col_character() ## ) ## # A tibble: 52 x 6 ## state med_income med_prop_val population mean_commute_minutes party ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 AK 64222 197300 733375 10.5 Republican ## 2 AL 36924 94800 4830620 25.3 Republican ## 3 AR 35833 83300 2958208 22.4 Republican ## 4 AZ 44748 128700 6641928 20.6 Republican ## 5 CA 53075 252100 38421464 23.4 Democrat ## 6 CO 48098 198900 5278906 19.5 Democrat ## 7 CT 69228 246450 3593222 24.3 Democrat ## 8 DC 70848 475800 647484 28.3 Democrat ## 9 DE 54976 228500 926454 24.5 Democrat ## 10 FL 43355 125600 19645772 24.8 Republican ## # … with 42 more rows Above you can also see something neat that Jupyter does to help us understand our code: it colours text depending on its meaning in R. For example, you’ll note that functions get bold green text, while letters and words surrounded by quotations like filenames get blue text. In case you want to know more (optional): We use the read_csv function from the tidyverse instead of the base R function read.csv because it’s faster and it creates a nicer variant of the base R data frame called a tibble. This has several benefits that we’ll discuss in further detail later in the course. 2.1 Assigning value to a data frame When we loaded the US state-level property, income, population, and voting data in R above using read_csv, we did not give this data frame a name, so it was just printed to the screen and we cannot do anything else with it. That isn’t very useful; what we would like to do is give a name to the data frame that read_csv outputs so that we can use it later for analysis and visualization. To assign name to something in R, there are two possible ways—using either the assignment symbol (&lt;-) or the equals symbol (=). From a style perspective, the assignment symbol is preferred and is what we will use in this course. When we name something in R using the assignment symbol, &lt;-, we do not need to surround it with quotes like the filename. This is because we are formally telling R about this word and giving it a value. Only characters and words that act as values need to be surrounded by quotes. Let’s now use the assignment symbol to give the name us_data to the US state-level property, income, population, and voting data frame that we get from read_csv. us_data &lt;- read_csv(&quot;state_property_vote.csv&quot;) Wait a minute! Nothing happened this time! Or at least it looks like that. But actually something did happen: the data was read in and now has the name us_data associated with it. And we can use that name to access the data frame and do things with it. First we will type the name of the data frame to print it to the screen. us_data ## # A tibble: 52 x 6 ## state med_income med_prop_val population mean_commute_minutes party ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 AK 64222 197300 733375 10.5 Republican ## 2 AL 36924 94800 4830620 25.3 Republican ## 3 AR 35833 83300 2958208 22.4 Republican ## 4 AZ 44748 128700 6641928 20.6 Republican ## 5 CA 53075 252100 38421464 23.4 Democrat ## 6 CO 48098 198900 5278906 19.5 Democrat ## 7 CT 69228 246450 3593222 24.3 Democrat ## 8 DC 70848 475800 647484 28.3 Democrat ## 9 DE 54976 228500 926454 24.5 Democrat ## 10 FL 43355 125600 19645772 24.8 Republican ## # … with 42 more rows 2.2 Creating subsets of data frames with select &amp; filter Now, we are going to learn how to obtain subsets of data from a data frame in R using two other tidyverse functions: select and filter. The select function allows you to create a subset of the columns of a data frame, while the filter function allows you to obtain a subset of the rows with specific values. Before we start using select and filter, let’s take a look at the US state-level property, income, and population data again to familiarize ourselves with it. We will do this by printing the data we loaded earlier in the chapter to the screen. us_data ## # A tibble: 52 x 6 ## state med_income med_prop_val population mean_commute_minutes party ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 AK 64222 197300 733375 10.5 Republican ## 2 AL 36924 94800 4830620 25.3 Republican ## 3 AR 35833 83300 2958208 22.4 Republican ## 4 AZ 44748 128700 6641928 20.6 Republican ## 5 CA 53075 252100 38421464 23.4 Democrat ## 6 CO 48098 198900 5278906 19.5 Democrat ## 7 CT 69228 246450 3593222 24.3 Democrat ## 8 DC 70848 475800 647484 28.3 Democrat ## 9 DE 54976 228500 926454 24.5 Democrat ## 10 FL 43355 125600 19645772 24.8 Republican ## # … with 42 more rows In this data frame there are 52 rows (corresponding to the 50 US states, the District of Columbia and the US territory, Puerto Rico) and 6 columns: US state abbreviation Median household income Median property value US state population Mean commute time in minutes The party each state voted for in the 2016 US presidential election Now let’s use select to extract the state column from this data frame. To do this, we need to provide the select function with two arguments. The first argument is the name of the data frame object, which in this example is us_data. The second argument is the column name that we want to select, here state. After passing these two arguments, the select function returns a single column (the state column that we asked for) as a data frame. state_column &lt;- select(us_data, state) state_column ## # A tibble: 52 x 1 ## state ## &lt;chr&gt; ## 1 AK ## 2 AL ## 3 AR ## 4 AZ ## 5 CA ## 6 CO ## 7 CT ## 8 DC ## 9 DE ## 10 FL ## # … with 42 more rows 2.2.1 Using select to extract multiple columns We can also use select to obtain a subset of the data frame with multiple columns. Again, the first argument is the name of the data frame. Then we list all the columns we want as arguments separated by commas. Here we create a subset of three columns: state, median property value, and mean commute time in minutes. three_columns &lt;- select(us_data, state, med_prop_val, mean_commute_minutes) three_columns ## # A tibble: 52 x 3 ## state med_prop_val mean_commute_minutes ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AK 197300 10.5 ## 2 AL 94800 25.3 ## 3 AR 83300 22.4 ## 4 AZ 128700 20.6 ## 5 CA 252100 23.4 ## 6 CO 198900 19.5 ## 7 CT 246450 24.3 ## 8 DC 475800 28.3 ## 9 DE 228500 24.5 ## 10 FL 125600 24.8 ## # … with 42 more rows 2.2.2 Using select to extract a range of columns We can also use select to obtain a subset of the data frame constructed from a range of columns. To do this we use the colon (:) operator to denote the range. For example, to get all the columns in the data frame from state to med_prop_val we pass state:med_prop_val as the second argument to the select function. column_range &lt;- select(us_data, state:med_prop_val) column_range ## # A tibble: 52 x 3 ## state med_income med_prop_val ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AK 64222 197300 ## 2 AL 36924 94800 ## 3 AR 35833 83300 ## 4 AZ 44748 128700 ## 5 CA 53075 252100 ## 6 CO 48098 198900 ## 7 CT 69228 246450 ## 8 DC 70848 475800 ## 9 DE 54976 228500 ## 10 FL 43355 125600 ## # … with 42 more rows 2.2.3 Using filter to extract a single row We can use the filter function to obtain the subset of rows with desired values from a data frame. Again, our first argument is the name of the data frame object, us_data. The second argument is a logical statement to use when filtering the rows. Here, for example, we’ll say that we are interested in rows where state equals NY (for New York). To make this comparison, we use the equivalency operator == to compare the values of the state column with the value \"NY\". Similar to when we loaded the data file and put quotes around the filename, here we need to put quotes around \"NY\" to tell R that this is a character value and not one of the special words that make up R programming language, nor one of the names we have given to data frames in the code we have already written. With these arguments, filter returns a data frame that has all the columns of the input data frame but only the rows we asked for in our logical filter statement. new_york &lt;- filter(us_data, state == &quot;NY&quot;) new_york ## # A tibble: 1 x 6 ## state med_income med_prop_val population mean_commute_minutes party ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 NY 50839 134150 19673174 24.4 Democrat 2.2.4 Using filter to extract rows with values above a threshold If we are interested in finding information about the states who have a longer mean commute time than New York—whose mean commute time is 21.5 minutes—then we can create a filter to obtain rows where the value of mean_commute_minutes is greater than 21.5. In this case, we see that filter returns a data frame with 33 rows; this indicates that there are 33 states with longer commute times on average than New York. long_commutes &lt;- filter(us_data, mean_commute_minutes &gt; 21.5) long_commutes ## # A tibble: 33 x 6 ## state med_income med_prop_val population mean_commute_minutes party ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 AL 36924 94800 4830620 25.3 Republican ## 2 AR 35833 83300 2958208 22.4 Republican ## 3 CA 53075 252100 38421464 23.4 Democrat ## 4 CT 69228 246450 3593222 24.3 Democrat ## 5 DC 70848 475800 647484 28.3 Democrat ## 6 DE 54976 228500 926454 24.5 Democrat ## 7 FL 43355 125600 19645772 24.8 Republican ## 8 GA 37865 101700 10006693 24.5 Republican ## 9 IL 47898 97350 12873761 22.6 Democrat ## 10 IN 47194 111800 6568645 23.5 Republican ## # … with 23 more rows 2.3 Exploring data with visualizations Creating effective data visualizations is an essential piece to any data analysis. For the remainder of Chapter 1, we will learn how to use functions from the tidyverse to make visualizations that let us explore relationships in data. In particular, we’ll develop a visualization of the US property, income, population, and voting data we’ve been working with that will help us understand two potential relationships in the data: first, the relationship between median household income and median propery value across the US, and second, whether there is a pattern in which party each state voted for in the 2016 US election. This is an example of an exploratory data analysis question: we are looking for relationships and patterns within the data set we have, but are not trying to generalize what we find beyond this data set. 2.3.1 Using ggplot to create a scatter plot Taking another look at our dataset below, we can immediately see that the three columns (or variables) we are interested in visualizing—median household income, median property value, and election result—are all in separate columns. In addition, there is a single row (or observation) for each state. The data are therefore in what we call a tidy data format. This is particularly important and will be a major focus in the remainder of this course: many of the functions from tidyverse require tidy data, including the ggplot function that we will use shortly for our visualization. Note below that we use the print function to display the us_data rather than just typing us_data; for data frames, these do the same thing. print(us_data) ## # A tibble: 52 x 6 ## state med_income med_prop_val population mean_commute_minutes party ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 AK 64222 197300 733375 10.5 Republican ## 2 AL 36924 94800 4830620 25.3 Republican ## 3 AR 35833 83300 2958208 22.4 Republican ## 4 AZ 44748 128700 6641928 20.6 Republican ## 5 CA 53075 252100 38421464 23.4 Democrat ## 6 CO 48098 198900 5278906 19.5 Democrat ## 7 CT 69228 246450 3593222 24.3 Democrat ## 8 DC 70848 475800 647484 28.3 Democrat ## 9 DE 54976 228500 926454 24.5 Democrat ## 10 FL 43355 125600 19645772 24.8 Republican ## # … with 42 more rows 2.3.2 Using ggplot to create a scatter plot We will begin with a scatter plot of the income and property value columns from our data frame. To create a scatter plot of these two variables using the ggplot function, we do the following: call the ggplot function provide the name of the data frame as the first argument call the aesthetic function, aes, to specify which column will correspond to the x-axis and which will correspond to the y-axis add a + symbol at the end of the ggplot call to add a layer to the plot call the geom_point function to tell R that we want to represent the data points as dots/points to create a scatter plot. ggplot(us_data, aes(x = med_income, y = med_prop_val)) + geom_point() In case you have used R before and are curious: There are a small number of situations in which you can have a single R expression span multiple lines. Here, the + symbol at the end of the first line tells R that the expression isn’t done yet and to continue reading on the next line. While not strictly necessary, this sort of pattern will appear a lot when using ggplot as it keeps things more readable. 2.3.3 Formatting ggplot objects One common and easy way to format your ggplot visualization is to add additional layers to the plot object using the + symbol. For example, we can use the xlab and ylab functions to add layers where we specify human readable labels for the x and y axes. Again, since we are specifying words (e.g. \"Income (USD)\") as arguments to xlab and ylab, we surround them with double quotes. There are many more layers we can add to format the plot further, and we will explore these in later chapters. ggplot(us_data, aes(x = med_income, y = med_prop_val)) + geom_point() + xlab(&quot;Income (USD)&quot;) + ylab(&quot;Median property value (USD)&quot;) From this visualization we see that for the 52 US regions in this data set, as median household income increases so does median property value. When we see two variables do this, we call this a positive relationship. Because the increasing pattern is fairly clear (not fuzzy) we can say that the relationship is strong. Because of the data point in the lower left-hand corner, drawing a straight line through these points wouldn’t fit very well. When a straight-line doesn’t fit the data well we say that it’s non-linear. However, we should have caution when using one point to claim non-linearity. As we will see later, this might be due to a single point not really belonging in the data set (this is often called an outlier). Learning how to describe data visualizations is a very useful skill. We will provide descriptions for you in this course (as we did above) until we get to Chapter 4, which focuses on data visualization. Then, we will explicitly teach you how to do this yourself, and how to not over-state or over-interpret the results from a visualization. 2.3.4 Coloring points by group Now we’ll move onto the second part of our exploratory data analysis question: when considering the relationship between median household income and median property value, is there a pattern in which party each state voted for in the 2016 US election? One common way to explore this is to colour the data points on the scatter plot we have already created by group/category. For example, given that we have the party each state voted for in the 2016 US Presidential election in the column named party, we can colour the points in our previous scatter plot to represent who each stated voted for. To do this we modify our scatter plot code above. Specifically, we will add an argument to the aes function, specifying that the points should be coloured by the party column: ggplot(us_data, aes(x = med_income, y = med_prop_val, color = party)) + geom_point() + xlab(&quot;Income (USD)&quot;) + ylab(&quot;Median property value (USD)&quot;) This data visualization shows that the one data point we singled out earlier on the far left of the plot has the label of “not applicable” instead of “democrat” or “republican”. Let’s use filter to look at the row that contains the “not applicable” value in the party column: missing_party &lt;- filter(us_data, party == &quot;Not Applicable&quot;) missing_party ## # A tibble: 0 x 6 ## # … with 6 variables: state &lt;chr&gt;, med_income &lt;dbl&gt;, med_prop_val &lt;dbl&gt;, population &lt;dbl&gt;, ## # mean_commute_minutes &lt;dbl&gt;, party &lt;fct&gt; That explains it! That row in the dataset is actually not a US state, but rather the US territory of Peurto Rico. Similar to other US territories, residents of Puerto Rico cannot vote in presidential elections. Hence the “not applicable” label. Let’s remove this row from the data frame and rename the data frame vote_data. To do this, we use the opposite of the equivalency operator (==) for our filter statement, the not equivalent operator (!=). vote_data &lt;- filter(us_data, party != &quot;Not Applicable&quot;) vote_data ## # A tibble: 51 x 6 ## state med_income med_prop_val population mean_commute_minutes party ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 AK 64222 197300 733375 10.5 Republican ## 2 AL 36924 94800 4830620 25.3 Republican ## 3 AR 35833 83300 2958208 22.4 Republican ## 4 AZ 44748 128700 6641928 20.6 Republican ## 5 CA 53075 252100 38421464 23.4 Democrat ## 6 CO 48098 198900 5278906 19.5 Democrat ## 7 CT 69228 246450 3593222 24.3 Democrat ## 8 DC 70848 475800 647484 28.3 Democrat ## 9 DE 54976 228500 926454 24.5 Democrat ## 10 FL 43355 125600 19645772 24.8 Republican ## # … with 41 more rows Now we see that the data frame has 51 rows corresponding to the 50 states and the District of Columbia - all regions where residents can vote in US presidential elections. Let’s now recreate the scatter plot we made above using this data frame subset: ggplot(vote_data, aes(x = med_income, y = med_prop_val, color = party)) + geom_point() + xlab(&quot;Income (USD)&quot;) + ylab(&quot;Median property value (USD)&quot;) What do we see when considering the second part of our exploratory question? Do we see a pattern in how certain states voted in the 2016 Presidential election? We do! Most of the US States who voted for the Republican candidate in the 2016 US Presidential election had lower median household income and lower median property values (data points primarily fall in lower left-hand side of the scatter plot), whereas most of the US states who voted for the Democratic candidate in the 2016 US Presidential election had higher median household income and higher median property values (data points primarily fall in the upper right-hand side of the scatter plot). Does this mean that rich states usually vote for Democrats and poorer states generally vote for Republicans? Or could we use this data visualization on its own to predict which party each state will vote for in the next presidential election? The answer to both these questions is “no.” What we can do with this exploratory data analysis is create new hypotheses, ideas, and questions (like the ones at the beginning of this paragraph). Answering those questions would likely involve gathering additional data and doing more complex analyses, which we will see more of later in this course. 2.3.5 Putting it all together Below, we put everything from this chapter together in one code chunk. This demonstrates the power of R: in relatively few lines of code, we are able to create an entire data science workflow. library(tidyverse) us_data &lt;- read_csv(&quot;state_property_vote.csv&quot;) vote_data &lt;- filter(us_data, party != &quot;Not Applicable&quot;) ggplot(vote_data, aes(x = med_income, y = med_prop_val, color = party)) + geom_point() + xlab(&quot;Income (USD)&quot;) + ylab(&quot;Median property value (USD)&quot;) 2.3.6 What’s next? In the next chapter, we will dig in and spend more time learning how to load spreadsheet-like datasets of various formats into R, as well as how to scrape data from the web! "],
["reading.html", "Chapter 3 Reading in data locally and from the web 3.1 Overview 3.2 Chapter learning objectives 3.3 Absolute and relative file paths 3.4 Reading tabular data from a plain text file into R 3.5 Reading data from an Microsoft Excel file 3.6 Reading data from a database 3.7 Writing data from R to a .csv file 3.8 Scraping data off the web using R 3.9 Additional readings/resources", " Chapter 3 Reading in data locally and from the web 3.1 Overview In this chapter, you’ll learn to read spreadsheet-like data of various formats into R from your local device and from the web. “Reading” (or “loading”) is the process of converting data (stored as plain text, a database, HTML, etc.) into an object (e.g., a dataframe) that R can easily access and manipulate, and is thus the gateway to any data analysis; you won’t be able to analyze data unless you’ve loaded it first. And because there are many ways to store data, there are similarly many ways to read data into R. If you spend more time upfront matching the data reading method to the type of data you have, you will have to spend less time re-formatting, cleaning and wrangling your data (the second step to all data analyses). It’s like making sure your shoelaces are tied well before going for a run so that you don’t trip later on! 3.2 Chapter learning objectives By the end of the chapter, students will be able to: define the following: absolute file path relative file path url read data into R using a relative path and a url compare and contrast the following functions: read_csv read_tsv read_csv2 read_delim read_excel match the following tidyverse read_* function arguments to their descriptions: file delim col_names skip choose the appropriate tidyverse read_* function and function arguments to load a given plain text tabular data set into R use readxl library’s read_excel function and arguments to load a sheet from an excel file into R connect to a database using the DBI library’s dbConnect function list the tables in a database using the DBI library’s dbListTables function create a reference to a database table that is queriable using the tbl from the dbplyr library retrieve data from a database query and bring it into R using the collect function from the dbplyr library (optional) scrape data from the web read/scrape data from an internet URL using the rvest html_nodes and html_text functions compare downloading tabular data from a plain text file (e.g. .csv) from the web versus scraping data from a .html file 3.3 Absolute and relative file paths When you load a data set into R, you first need to tell R where that files lives. The file could live on your computer (local), or somewhere on the internet (remote). In this section we will discuss the case where the file lives on your computer. The place where the file lives on your computer is called the “path”. You can think of the path as directions to the file. There are two kinds of paths: relative paths and absolute paths. A relative path is where the file is in respect to where you currently are on the computer (e.g., where the Jupyter notebook file that you’re working in is). On the other hand, an absolute path is where the file is in respect to the base (or root) folder of the computer’s filesystem. Suppose our computer’s filesystem looks like the picture below, and we are working in the Jupyter notebook titled worksheetk_02.ipynb. If we want to read in the .csv file named happiness_report.csv into our Jupyter notebook using R, we could do this using either a relative or an absolute path. We show what both choices below. Reading happiness_report.csv using a relative path: happiness_data &lt;- read_csv(&quot;data/happiness_report.csv&quot;) Reading happiness_report.csv using an absolute path: happiness_data &lt;- read_csv(&quot;/home/jupyter/dsci-100/worksheet_02/data/happiness_report.csv&quot;) So which one should you use? Generally speaking, to ensure your code can be run on a different computer, you should use relative paths (and it’s also less typing!). This is because the absolute path of a file (the names of folders between the computer’s root / and the file) isn’t usually the same across multiple computers. For example, suppose Alice and Bob are working on a project together on the happiness_report.csv data. Alice’s file is stored at /home/alice/project/data/happiness_report.csv, while Bob’s is stored at /home/bob/project/data/happiness_report.csv. Even though Alice and Bob stored their files in the same place on their computers (in their home folders), the absolute paths are different due to their different usernames. If Bob has code that loads the happiness_report.csv data using an absolute path, the code won’t work on Alice’s computer. But the relative path from inside the project folder (data/happiness_report.csv) is the same on both computers; any code that uses relative paths will work on both! See this video for another explanation: Source: Udacity course “Linux Command Line Basics” 3.4 Reading tabular data from a plain text file into R Now we will learn more about reading tabular data from a plain text file into R, as well as how to write tabular data to a file. Last chapter we learned about using the tidyverse read_csv function when reading files that match that functions expected defaults (column names are present and commas are used as the delimiter/separator between columns). In this section, we will learn how to read files do not satisfy the default expectations of read_csv. Before we jump into the cases where the data aren’t in the expected default format for tidyverse and read_csv, let’s revisit the simpler case where the defaults hold and the only argument we need to give to the function is the path to the file, data/state_property_vote.csv. We put data/ before the name of the file when we are loading the dataset because this dataset is located in a sub-folder, named data, relative to where we are running our R code. Here is what the file would look in a plain text editor: state,med_income,med_prop_val,population,mean_commute_minutes,party AK,64222,197300,733375,10.46830207,Republican AL,36924,94800,4830620,25.30990746,Republican AR,35833,83300,2958208,22.40108933,Republican AZ,44748,128700,6641928,20.58786,Republican CA,53075,252100,38421464,23.38085172,Democrat CO,48098,198900,5278906,19.50792188,Democrat CT,69228,246450,3593222,24.349675,Democrat DC,70848,475800,647484,28.2534,Democrat DE,54976,228500,926454,24.45553333,Democrat And here is a review of how we can use read_csv to load it into R: library(tidyverse) us_data &lt;- read_csv(&quot;data/state_property_vote.csv&quot;) us_data ## # A tibble: 52 x 6 ## state med_income med_prop_val population mean_commute_minutes party ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 AK 64222 197300 733375 10.5 Republican ## 2 AL 36924 94800 4830620 25.3 Republican ## 3 AR 35833 83300 2958208 22.4 Republican ## 4 AZ 44748 128700 6641928 20.6 Republican ## 5 CA 53075 252100 38421464 23.4 Democrat ## 6 CO 48098 198900 5278906 19.5 Democrat ## 7 CT 69228 246450 3593222 24.3 Democrat ## 8 DC 70848 475800 647484 28.3 Democrat ## 9 DE 54976 228500 926454 24.5 Democrat ## 10 FL 43355 125600 19645772 24.8 Republican ## # … with 42 more rows 3.4.1 Skipping rows when reading in data Often times information about how data was collected, or other relevant information, is included at the top of the data file. This information is usually written in sentence and paragraph form, with no delimiter because it is not organized into columns. An example of this is shown below: Data source: https://datausa.io/ Record of how data was collected: https://github.com/UBC-DSCI/introduction-to-datascience/blob/master/data/src/retrieve_data_usa.ipynb Date collected: 2017-06-06 state,med_income,med_prop_val,population,mean_commute_minutes,party AK,64222,197300,733375,10.46830207,Republican AL,36924,94800,4830620,25.30990746,Republican AR,35833,83300,2958208,22.40108933,Republican AZ,44748,128700,6641928,20.58786,Republican CA,53075,252100,38421464,23.38085172,Democrat CO,48098,198900,5278906,19.50792188,Democrat CT,69228,246450,3593222,24.349675,Democrat DC,70848,475800,647484,28.2534,Democrat DE,54976,228500,926454,24.45553333,Democrat Using read_csv as we did previously does not allow us to correctly load the data into R. In the case of this file we end up only reading in one column of the data set: us_data &lt;- read_csv(&quot;data/state_property_vote_meta-data.csv&quot;) ## Parsed with column specification: ## cols( ## `Data source: https://datausa.io/` = col_character() ## ) us_data ## # A tibble: 55 x 1 ## `Data source: https://datausa.io/` ## &lt;chr&gt; ## 1 Record of how data was collected: https://github.com/UBC-DSCI/introduction-to-datascience/blob/mast… ## 2 Date collected: 2017-06-06 ## 3 state ## 4 AK ## 5 AL ## 6 AR ## 7 AZ ## 8 CA ## 9 CO ## 10 CT ## # … with 45 more rows To successfully read data like this into R, the skip argument can be useful to tell R how many lines to skip before it should start reading in the data. In the example above, we would set this value to 3: us_data &lt;- read_csv(&quot;data/state_property_vote_meta-data.csv&quot;, skip = 3) ## Parsed with column specification: ## cols( ## state = col_character(), ## med_income = col_double(), ## med_prop_val = col_double(), ## population = col_double(), ## mean_commute_minutes = col_double(), ## party = col_character() ## ) us_data ## # A tibble: 52 x 6 ## state med_income med_prop_val population mean_commute_minutes party ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 AK 64222 197300 733375 10.5 Republican ## 2 AL 36924 94800 4830620 25.3 Republican ## 3 AR 35833 83300 2958208 22.4 Republican ## 4 AZ 44748 128700 6641928 20.6 Republican ## 5 CA 53075 252100 38421464 23.4 Democrat ## 6 CO 48098 198900 5278906 19.5 Democrat ## 7 CT 69228 246450 3593222 24.3 Democrat ## 8 DC 70848 475800 647484 28.3 Democrat ## 9 DE 54976 228500 926454 24.5 Democrat ## 10 FL 43355 125600 19645772 24.8 Republican ## # … with 42 more rows 3.4.2 read_delim as a more flexible method to get tabular data into R When our tabular data comes in a different format, we can use the read_delim function instead. For example, a different version of this same dataset has no column names and uses tabs as the delimiter instead of commas. Here is how the file would look in a plain text editor: AK 64222 197300 733375 10.46830207 Republican AL 36924 94800 4830620 25.30990746 Republican AR 35833 83300 2958208 22.40108933 Republican AZ 44748 128700 6641928 20.58786 Republican CA 53075 252100 38421464 23.38085172 Democrat CO 48098 198900 5278906 19.50792188 Democrat CT 69228 246450 3593222 24.349675 Democrat DC 70848 475800 647484 28.2534 Democrat DE 54976 228500 926454 24.45553333 Democrat FL 43355 125600 19645772 24.78055522 Republican To get this into R using the read_delim() function, we specify the first argument as the path to the file (as done with read_csv), and then provide values to the delim argument (here a tab, which we represent by \"\\t\") and the col_names argument (here we specify that there are no column names be assigning it the value of FALSE). Both read_csv() and read_delim() have a col_names argument and the default is TRUE. us_data &lt;- read_delim(&quot;data/state_property_vote.tsv&quot;, delim = &quot;\\t&quot;, col_names = FALSE) ## Parsed with column specification: ## cols( ## X1 = col_character(), ## X2 = col_double(), ## X3 = col_double(), ## X4 = col_double(), ## X5 = col_double(), ## X6 = col_character() ## ) us_data ## # A tibble: 52 x 6 ## X1 X2 X3 X4 X5 X6 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 AK 64222 197300 733375 10.5 Republican ## 2 AL 36924 94800 4830620 25.3 Republican ## 3 AR 35833 83300 2958208 22.4 Republican ## 4 AZ 44748 128700 6641928 20.6 Republican ## 5 CA 53075 252100 38421464 23.4 Democrat ## 6 CO 48098 198900 5278906 19.5 Democrat ## 7 CT 69228 246450 3593222 24.3 Democrat ## 8 DC 70848 475800 647484 28.3 Democrat ## 9 DE 54976 228500 926454 24.5 Democrat ## 10 FL 43355 125600 19645772 24.8 Republican ## # … with 42 more rows Data frames in R need to have column names, thus if you read data into R as a data frame without column names then R assigns column names for them. If you used the read_* functions to read the data into R, then R gives each column a name of X1, X2, …, XN, where N is the number of columns in the data set. 3.4.3 Reading tabular data directly from a URL We can also use read_csv() or read_delim() (and related functions) to read in tabular data directly from a url that contains tabular data. In this case, we provide the url to read_csv() as the path to the file instead of a path to a local file on our computer. Similar to when we specify a path on our local computer, here we need to surround the url by quotes. All other arguments that we use are the same as when using these functions with a local file on our computer. us_data &lt;- read_csv(&quot;https://raw.githubusercontent.com/UBC-DSCI/introduction-to-datascience/master/state_property_vote.csv&quot;) ## Parsed with column specification: ## cols( ## state = col_character(), ## med_income = col_double(), ## med_prop_val = col_double(), ## population = col_double(), ## mean_commute_minutes = col_double(), ## party = col_character() ## ) us_data ## # A tibble: 52 x 6 ## state med_income med_prop_val population mean_commute_minutes party ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 AK 64222 197300 733375 10.5 Republican ## 2 AL 36924 94800 4830620 25.3 Republican ## 3 AR 35833 83300 2958208 22.4 Republican ## 4 AZ 44748 128700 6641928 20.6 Republican ## 5 CA 53075 252100 38421464 23.4 Democrat ## 6 CO 48098 198900 5278906 19.5 Democrat ## 7 CT 69228 246450 3593222 24.3 Democrat ## 8 DC 70848 475800 647484 28.3 Democrat ## 9 DE 54976 228500 926454 24.5 Democrat ## 10 FL 43355 125600 19645772 24.8 Republican ## # … with 42 more rows 3.4.4 Previewing a data file before reading it into R In all the examples above, we gave you previews of the data file before we read it into R. This is essential so that you can see whether or not there are column names, what the delimiters are, and if there are lines you need to skip. You should do this yourself when trying to read in data files. In Jupyter, you preview data as a plain text file by clicking on the file’s name in the Jupyter home menu. We demonstrate this in the video below: 3.5 Reading data from an Microsoft Excel file There are many other ways to store tabular datasets beyond plain text files, and similarly many ways to load those datasets into R. For example, it is very common to encounter, and need to load into R, data stored as a Microsoft Excel spreadsheet (with the filename extension .xlsx). To be able to do this, a key thing to know is that even though .csv and .xlsx files look almost identical when loaded into Excel, the data themselves are stored completely differently. While .csv files are plain text files, where the characters you see when you open the file in a text editor are exactly the data they represent, this is not the case for .xlsx files. Take a look at what a .xlsx file would look like in a text editor: ,?&#39;O _rels/.rels???J1??&gt;E?{7? &lt;?V????w8?&#39;J???&#39;QrJ???Tf?d??d?o?wZ&#39;???@&gt;?4&#39;?|??hlIo??F t 8f??3wn ????t??u&quot;/ %~Ed2??&lt;?w?? ?Pd(??J-?E???7?&#39;t(?-GZ?????y???c~N?g[^_r?4 yG?O ?K??G?RPX?&lt;??,?&#39;O[Content_Types].xml???n?0E%?J ]TUEe??O??c[???????6q??s??d?m???\\???H?^????3} ?rZY? ?:L60?^?????XTP+?|?3???&quot;~?3T1W3???,?#p?R?!??w(??R???[S?D?kP?P!XS(?i?t?$?ei X?a??4VT?,D?Jq D ?????u?]??;??L?.8AhfNv}?hHF*??Jr?Q?%?g?U??CtX&quot;8x&gt;?.|????5j?/$???JE?c??~??4iw?????E;?+?S??w?cV+?:???2l???=?2nel???;|?V??????c&#39;?????9?P&amp;Bcj,?&#39;OdocProps/app.xml??1 ?0???k????A?u?U?]??{#?:;/&lt;?g?Cd????M+?=???Z?O??R+??u?P?X KV@??M$??a???d?_???4??5v?R????9D????t??Fk?Ú&#39;P?=?,?&#39;OdocProps/core.xml??MO?0 ??J?{???3j?h&#39;??(q??U4J ??=i?I&#39;?b??[v?!??{gk? F2????v5yj??&quot;J???,?d???J???C??l??4?-?`$?4t?K?.;?%c?J??G&lt;?H???? X????z???6?????~q??X??????q^&gt;??tH???*?D???M?g ??D?????????d?:g).?3.??j?P?F?&#39;Oxl/_rels/workbook.xml.rels??Ak1??J?{7???R?^J?kk@Hf7??I?L???E]A?Þ?{a??`f?????b?6xUQ?@o?m}??o????X{???Q?????;?y?\\? O ?YY??4?L??S??k?252j?? ??V ?C?g?C]??????? ? ???E??TENyf6% ?Y????|??:%???}^ N?Q?N&#39;????)??F?\\??P?G??,?&#39;O&#39;xl/printerSettings/printerSettings1.bin?Wmn? ??Sp&gt;?G???q?# ?I??5R&#39;???q????(?L ??m??8F?5&lt; L`??`?A??2{dp??9R#?&gt;7??Xu???/?X??HI?|? ??r)???\\?VA8?2dFfq???I]]o 5`????6A ? This type of file representation allows Excel files to store additional things that you cannot store in a .csv file, such as fonts, text formatting, graphics, multiple sheets and more. And despite looking odd in a plain text editor, we can read Excel spreadsheets into R using the readxl package developed specifically for this purpose. library(readxl) us_data &lt;- read_excel(&quot;data/state_property_vote.xlsx&quot;) us_data ## # A tibble: 52 x 6 ## state med_income med_prop_val population mean_commute_minutes party ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 AK 64222 197300 733375 10.5 Republican ## 2 AL 36924 94800 4830620 25.3 Republican ## 3 AR 35833 83300 2958208 22.4 Republican ## 4 AZ 44748 128700 6641928 20.6 Republican ## 5 CA 53075 252100 38421464 23.4 Democrat ## 6 CO 48098 198900 5278906 19.5 Democrat ## 7 CT 69228 246450 3593222 24.3 Democrat ## 8 DC 70848 475800 647484 28.3 Democrat ## 9 DE 54976 228500 926454 24.5 Democrat ## 10 FL 43355 125600 19645772 24.8 Republican ## # … with 42 more rows If the .xlsx file has multiple sheets, then you have to use the sheet argument to specify either the sheet number or name. You can also specify cell ranges using the range argument. This is useful in cases where a single sheet contains multiple tables (a sad thing that happens to many Excel spreadsheets). As with plain text files, you should always try to explore the data file before importing it into R. This helps you decide which arguments you need to use to successfully load the data into R. If you do not have the Excel program on your computer, there are other free programs you can use to preview the file. Examples include Google Sheets and Libre Office. 3.6 Reading data from a database Another very common form of data storage to be read into R for the purpose of data analysis is the relational database. There are many relational database management systems, such as SQLite, MySQL, PosgreSQL, Oracle, and many more. Almost all employ SQL (structured query language) to pull data from the database. Thankfully, you don’t need to know SQL to analyze data from a database; several packages have been written that allow R to connect to relational databases and use the R programming language as the front end (what the user types in) to pull data from them. In this book we will give examples of how to do this using R with SQLite and PostgreSQL databases. 3.6.1 Reading data from a SQLite database SQLite is probably the simplest relational database that one can use in combination with R. SQLite databases are self-contained and usually stored and accessed locally on one computer. Data is usually stored in a file with a .db extension. Similar to Excel files, these are not plain text files and cannot be read in a plain text editor. The first thing you need to do to read data into R from a database is to connect to the database. We do that using the dbConnect function from the DBI (database interface) package. This does not read in the data, but simply tells R where the database is and opens up a communication channel. library(DBI) con_state_data &lt;- dbConnect(RSQLite::SQLite(), &quot;data/state_property_vote.db&quot;) Often times relational databases have many tables, and their power comes from the useful ways they can be joined. Thus anytime you want to access data from a relational database you need to know the table names. You can get the names of all the tables in the database using the dbListTables function: tables &lt;- dbListTables(con_state_data) tables ## [1] &quot;state&quot; We only get one table name returned form calling dbListTables, and this tells us that there is only one table in this database. To reference a table in the database so we can do things like select columns and filter rows, we use the tbl function from the dbplyr package: library(dbplyr) state_db &lt;- tbl(con_state_data, &quot;state&quot;) state_db ## # Source: table&lt;state&gt; [?? x 6] ## # Database: sqlite 3.31.1 ## # [/Users/tiffany/Desktop/introduction-to-datascience/data/state_property_vote.db] ## state med_income med_prop_val population mean_commute_minutes party ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 AK 64222 197300 733375 10.5 Republican ## 2 AL 36924 94800 4830620 25.3 Republican ## 3 AR 35833 83300 2958208 22.4 Republican ## 4 AZ 44748 128700 6641928 20.6 Republican ## 5 CA 53075 252100 38421464 23.4 Democrat ## 6 CO 48098 198900 5278906 19.5 Democrat ## 7 CT 69228 246450 3593222 24.3 Democrat ## 8 DC 70848 475800 647484 28.3 Democrat ## 9 DE 54976 228500 926454 24.5 Democrat ## 10 FL 43355 125600 19645772 24.8 Republican ## # … with more rows Although it looks like we just got a data frame from the database, we didn’t! It’s a reference, showing us data that is still in the SQLite database (note the first two lines of the output). It does this because databases are often more efficient at selecting, filtering and joining large datasets than R. And typically, the database will not even be stored on your computer, but rather a more powerful machine somewhere on the web. So R is lazy and waits to bring this data into memory until you explicitly tell it to do so using the collect function from the dbplyr library. Here we will filter for only states that voted for the Republican candidate in the 2016 Presidential election, and then use collect to finally bring this data into R as a data frame. republican_db &lt;- filter(state_db, party == &quot;Republican&quot;) republican_db ## # Source: lazy query [?? x 6] ## # Database: sqlite 3.31.1 ## # [/Users/tiffany/Desktop/introduction-to-datascience/data/state_property_vote.db] ## state med_income med_prop_val population mean_commute_minutes party ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 AK 64222 197300 733375 10.5 Republican ## 2 AL 36924 94800 4830620 25.3 Republican ## 3 AR 35833 83300 2958208 22.4 Republican ## 4 AZ 44748 128700 6641928 20.6 Republican ## 5 FL 43355 125600 19645772 24.8 Republican ## 6 GA 37865 101700 10006693 24.5 Republican ## 7 IA 49448 102700 3093526 18.4 Republican ## 8 ID 43080. 143900 1616547 19.9 Republican ## 9 IN 47194 111800 6568645 23.5 Republican ## 10 KS 46875 85200 2892987 16.7 Republican ## # … with more rows republican_data &lt;- collect(republican_db) republican_data ## # A tibble: 31 x 6 ## state med_income med_prop_val population mean_commute_minutes party ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 AK 64222 197300 733375 10.5 Republican ## 2 AL 36924 94800 4830620 25.3 Republican ## 3 AR 35833 83300 2958208 22.4 Republican ## 4 AZ 44748 128700 6641928 20.6 Republican ## 5 FL 43355 125600 19645772 24.8 Republican ## 6 GA 37865 101700 10006693 24.5 Republican ## 7 IA 49448 102700 3093526 18.4 Republican ## 8 ID 43080. 143900 1616547 19.9 Republican ## 9 IN 47194 111800 6568645 23.5 Republican ## 10 KS 46875 85200 2892987 16.7 Republican ## # … with 21 more rows Why bother to use the collect function? The data looks pretty similar in both outputs shown above. And dbplyr provides lots of functions similar to filter that you can use to directly feed the database reference (what tbl gives you) into downstream analysis functions (e.g., ggplot2 for data visualization and lm for linear regression modeling). However, this does not work in every case; look what happens when we try to use nrow to count rows in a data frame: nrow(republican_db) ## [1] NA or tail to preview the last 6 rows of a data frame: tail(republican_db) ## Error: tail() is not supported by sql sources Additionally, some operations will not work to extract columns or single values from the reference given by the tbl function. Thus, once you have finished your data wrangling of the tbl database reference object, it is advisable to then bring it into your local machine’s memory using collect as a data frame. 3.6.2 Reading data from a PostgreSQL database PostgreSQL (also called Postgres) is a very popular free and open-source option for relational database software. Unlike SQLite, PostgreSQL uses a client–server database engine, as it was designed to be used and accessed on a network. This means that you have to provide more information to R when connecting to Postgres databases. The additional information that you need to include when you call the dbConnect function is listed below: dbname - the name of the database (a single PostgreSQL instance can host more than one database) host - the URL pointing to where the database is located port - the communication endpoint between R and the PostgreSQL database (this is typically 5432 for PostgreSQL) user - the username for accessing the database password - the password for accessing the database Additionally, we must use the RPostgres library instead of RSQLite in the dbConnect function call. Below we demonstrate how to connect to a version of the can_mov_db database, which contains information about Canadian movies (note - this is a synthetic, or artificial, database). library(RPostgres) can_mov_db_con &lt;- dbConnect(RPostgres::Postgres(), dbname = &quot;can_mov_db&quot;, host = &quot;r7k3-mds1.stat.ubc.ca&quot;, port = 5432, user = &quot;user0001&quot;, password = &#39;################&#39;) After opening the connection, everything looks and behaves almost identically to when we were using an SQLite database in R. For example, we can again use dbListTables to find out what tables are in the can_mov_db database: dbListTables(can_mov_db_con) [1] &quot;themes&quot; &quot;medium&quot; &quot;titles&quot; &quot;title_aliases&quot; &quot;forms&quot; [6] &quot;episodes&quot; &quot;names&quot; &quot;names_occupations&quot; &quot;occupation&quot; &quot;ratings&quot; We see that there are 10 tables in this database. Let’s first look at the \"ratings\" table to find the lowest rating that exists in the can_mov_db database: ratings_db &lt;- tbl(can_mov_db_con, &quot;ratings&quot;) ratings_db # Source: table&lt;ratings&gt; [?? x 3] # Database: postgres [user0001@r7k3-mds1.stat.ubc.ca:5432/can_mov_db] title average_rating num_votes &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; 1 The Grand Seduction 6.6 150 2 Rhymes for Young Ghouls 6.3 1685 3 Mommy 7.5 1060 4 Incendies 6.1 1101 5 Bon Cop, Bad Cop 7.0 894 6 Goon 5.5 1111 7 Monsieur Lazhar 5.6 610 8 What if 5.3 1401 9 The Barbarian Invations 5.8 99 10 Away from Her 6.9 2311 # … with more rows To find the lowest rating that exists in the data base, we first need to extract the average_rating column using select: avg_rating_db &lt;- select(ratings_db, average_rating) avg_rating_db # Source: lazy query [?? x 1] # Database: postgres [user0001@r7k3-mds1.stat.ubc.ca:5432/can_mov_db] average_rating &lt;dbl&gt; 1 6.6 2 6.3 3 7.5 4 6.1 5 7.0 6 5.5 7 5.6 8 5.3 9 5.8 10 6.9 # … with more rows Next we use min to find the minimum rating in that column: min(avg_rating_db) Error in min(avg_rating_db) : invalid &#39;type&#39; (list) of argument Instead of the minimum, we get an error! This is another example of when we need to use the collect function to bring the data into R for further computation: avg_rating_data &lt;- collect(avg_rating_db) min(avg_rating_data) [1] 1 We see the lowest rating given to a movie is 1, indicating that it must have been a really bad movie… 3.7 Writing data from R to a .csv file At the middle and end of a data analysis, we often want to write a data frame that has changed (either through filtering, selecting, mutating or summarizing) to a file so that we can share it with others or use it for another step in the analysis. The most straightforward way to do this is to use the write_csv function from the tidyverse library. The default arguments for this file are to use a comma (,) as the delimiter and include column names. Below we demonstrate creating a new version of the US state-level property, income, population and voting data from 2015 and 2016 that does not contain the territory of Puerto Rico, and then writing this to a .csv file: state_data &lt;- filter(us_data, state != &quot;PR&quot;) write_csv(state_data, &quot;data/us_states_only.csv&quot;) 3.8 Scraping data off the web using R In the first part of this chapter we learned how to read in data from plain text files that are usually “rectangular” in shape using the tidyverse read_* functions. Sadly, not all data comes in this simple format, but happily there are many other tools we can use to read in more messy/wild data formats. One common place people often want/need to read in data from is websites. Such data exists in an a non-rectangular format. One quick and easy solution to get this data is to copy and paste it, however this becomes painstakingly long and boring when there is a lot of data that needs gathering, and anytime you start doing a lot of copying and pasting it is very likely you will introduce errors. The formal name for gathering non-rectangular data from the web and transforming it into a more useful format for data analysis is web scraping. There are two different ways to do web scraping: 1) screen scraping (similar to copying and pasting from a website, but done in a programmatic way to minimize errors and maximize efficiency) and 2) web APIs (application programming interface) (a website that provides a programatic way of returning the data as JSON or XML files via http requests). In this course we will explore the first method, screen scraping using R’s rvest package. 3.8.1 HTML and CSS selectors Before we jump into scraping, let’s set up some motivation and learn a little bit about what the “source code” of a website looks like. Say we are interested in knowing the average rental price (per square footage) of the most recently available 1 bedroom apartments in Vancouver from https://vancouver.craigslist.org. When we visit the Vancouver Craigslist website and search for 1 bedroom apartments, this is what we are shown: From that page, it’s pretty easy for our human eyes to find the apartment price and square footage. But how can we do this programmatically so we don’t have to copy and paste all these numbers? Well, we have to deal with the webpage source code, which we show a snippet of below (and link to the entire source code here): &lt;span class=&quot;result-meta&quot;&gt; &lt;span class=&quot;result-price&quot;&gt;$800&lt;/span&gt; &lt;span class=&quot;housing&quot;&gt; 1br - &lt;/span&gt; &lt;span class=&quot;result-hood&quot;&gt; (13768 108th Avenue)&lt;/span&gt; &lt;span class=&quot;result-tags&quot;&gt; &lt;span class=&quot;maptag&quot; data-pid=&quot;6786042973&quot;&gt;map&lt;/span&gt; &lt;/span&gt; &lt;span class=&quot;banish icon icon-trash&quot; role=&quot;button&quot;&gt; &lt;span class=&quot;screen-reader-text&quot;&gt;hide this posting&lt;/span&gt; &lt;/span&gt; &lt;span class=&quot;unbanish icon icon-trash red&quot; role=&quot;button&quot; aria-hidden=&quot;true&quot;&gt;&lt;/span&gt; &lt;a href=&quot;#&quot; class=&quot;restore-link&quot;&gt; &lt;span class=&quot;restore-narrow-text&quot;&gt;restore&lt;/span&gt; &lt;span class=&quot;restore-wide-text&quot;&gt;restore this posting&lt;/span&gt; &lt;/a&gt; &lt;/span&gt; &lt;/p&gt; &lt;/li&gt; &lt;li class=&quot;result-row&quot; data-pid=&quot;6788463837&quot;&gt; &lt;a href=&quot;https://vancouver.craigslist.org/nvn/apa/d/north-vancouver-luxury-1-bedroom/6788463837.html&quot; class=&quot;result-image gallery&quot; data-ids=&quot;1:00U0U_lLWbuS4jBYN,1:00T0T_9JYt6togdOB,1:00r0r_hlMkwxKqoeq,1:00n0n_2U8StpqVRYX,1:00M0M_e93iEG4BRAu,1:00a0a_PaOxz3JIfI,1:00o0o_4VznEcB0NC5,1:00V0V_1xyllKkwa9A,1:00G0G_lufKMygCGj6,1:00202_lutoxKbVTcP,1:00R0R_cQFYHDzGrOK,1:00000_hTXSBn1SrQN,1:00r0r_2toXdps0bT1,1:01616_dbAnv07FaE7,1:00g0g_1yOIckt0O1h,1:00m0m_a9fAvCYmO9L,1:00C0C_8EO8Yl1ELUi,1:00I0I_iL6IqV8n5MB,1:00b0b_c5e1FbpbWUZ,1:01717_6lFcmuJ2glV&quot;&gt; &lt;span class=&quot;result-price&quot;&gt;$2285&lt;/span&gt; &lt;/a&gt; &lt;p class=&quot;result-info&quot;&gt; &lt;span class=&quot;icon icon-star&quot; role=&quot;button&quot;&gt; &lt;span class=&quot;screen-reader-text&quot;&gt;favorite this post&lt;/span&gt; &lt;/span&gt; &lt;time class=&quot;result-date&quot; datetime=&quot;2019-01-06 12:06&quot; title=&quot;Sun 06 Jan 12:06:01 PM&quot;&gt;Jan 6&lt;/time&gt; &lt;a href=&quot;https://vancouver.craigslist.org/nvn/apa/d/north-vancouver-luxury-1-bedroom/6788463837.html&quot; data-id=&quot;6788463837&quot; class=&quot;result-title hdrlnk&quot;&gt;Luxury 1 Bedroom CentreView with View - Lonsdale&lt;/a&gt; This is not easy for our human eyeballs to read! However, it is easy for us to use programmatic tools to extract the data we need by specifying which HTML tags (things inside &lt; and &gt; in the code above). For example, if we look in the code above and search for lines with a price, we can also look at the tags that are near that price and see if there’s a common “word” we can use that is near the price but doesn’t exist on other lines that have information we are not interested in: &lt;span class=&quot;result-price&quot;&gt;$800&lt;/span&gt; and &lt;span class=&quot;result-price&quot;&gt;$2285&lt;/span&gt; What we can see is there is a special “word” here, “result-price”, which appears only on the lines with prices and not on the other lines (that have information we are not interested in). This special word and the context in which is is used (learned from the other words inside the HTML tag) can be combined to create something called a CSS selector. The CSS selector can then be used by R’s rvest package to select the information we want (here price) from the website source code. Now, many websites are quite large and complex, and so then is their website source code. And as you saw above, it is not easy to read and pick out the special words we want with our human eyeballs. So to make this easier, we will use the SelectorGadget tool. It is an open source tool that simplifies generating and finding CSS selectors. We recommend you use the Chrome web browser to use this tool, and install the selector gadget tool from the Chrome Web Store. Here is a short video on how to install and use the SelectorGadget tool to get a CSS selector for use in web scraping: From installing and using the selectorgadget as shown in the video above, we get the two CSS selectors .housing and .result-price that we can use to scrape information about the square footage and the rental price, respectively. The selector gadget returns them to us as a comma separated list (here .housing , .result-price), which is exactly the format we need to provide to R if we are using more than one CSS selector. 3.8.2 Are you allowed to scrape that website? BEFORE scraping data from the web, you should always check whether or not you are ALLOWED to scrape it! There are two documents that are important for this: the robots.txt file and reading the website’s Terms of Service document. The website’s Terms of Service document is probably the more important of the two, and so you should look there first. What happens when we look at Craigslist’s Terms of Service document? Well we read this: “You agree not to copy/collect CL content via robots, spiders, scripts, scrapers, crawlers, or any automated or manual equivalent (e.g., by hand).” source: https://www.craigslist.org/about/terms.of.use Want to learn more about the legalities of web scraping and crawling? Read this interesting blog post titled “Web Scraping and Crawling Are Perfectly Legal, Right?” by Benoit Bernard (this is optional, not required reading). So what to do now? Well, we shouldn’t scrape Craigslist! Let’s instead scrape some data on the population of Canadian cities from Wikipedia (who’s Terms of Service document does not explicilty say do not scrape). In this video below we demonstrate using the selectorgadget tool to get CSS Selectors from Wikipedia’s Canada page to scrape a table that contains city names and their populations from the 2016 Canadian Census: 3.8.3 Using rvest Now that we have our CSS selectors we can use rvest R package to scrape our desired data from the website. First we start by loading the rvest package: library(rvest) library(rvest) gives error… If you get an error about R not being able to find the package (e.g., Error in library(rvest) : there is no package called ‘rvest’) this is likely because it was not installed. To install the rvest package, run the following command once inside R (and then delete that line of code): install.packages(\"rvest\"). Next, we tell R what page we want to scrape by providing the webpage’s URL in quotations to the function read_html: page &lt;- read_html(&quot;https://en.wikipedia.org/wiki/Canada&quot;) Then we send the page object to the html_nodes function. We also provide that function with the CSS selectors we obtained from the selectorgadget tool. These should be surrounded by quotations. The html_nodes function select nodes from the HTML document using CSS selectors. nodes are the HTML tag pairs as well as the content between the tags. For our CSS selector td:nth-child(5) and example node that would be selected would be: &lt;td style=\"text-align:left;background:#f0f0f0;\"&gt;&lt;a href=\"/wiki/London,_Ontario\" title=\"London, Ontario\"&gt;London&lt;/a&gt;&lt;/td&gt; population_nodes &lt;- html_nodes(page, &quot;td:nth-child(5) , td:nth-child(7) , .infobox:nth-child(122) td:nth-child(1) , .infobox td:nth-child(3)&quot;) head(population_nodes) ## {xml_nodeset (6)} ## [1] &lt;td style=&quot;text-align:right;&quot;&gt;5,928,040&lt;/td&gt; ## [2] &lt;td style=&quot;text-align:left;background:#f0f0f0;&quot;&gt;&lt;a href=&quot;/wiki/London,_Ontario&quot; title=&quot;London, O ... ## [3] &lt;td style=&quot;text-align:right;&quot;&gt;494,069\\n&lt;/td&gt; ## [4] &lt;td style=&quot;text-align:right;&quot;&gt;4,098,927&lt;/td&gt; ## [5] &lt;td style=&quot;text-align:left;background:#f0f0f0;&quot;&gt;\\n&lt;a href=&quot;/wiki/St._Catharines&quot; title=&quot;St. Cath ... ## [6] &lt;td style=&quot;text-align:right;&quot;&gt;406,074\\n&lt;/td&gt; Next we extract the meaningful data from the HTML nodes using the html_text function. For our example, this functions only required argument is the an html_nodes object, which we named rent_nodes. In the case of this example node: &lt;td style=\"text-align:left;background:#f0f0f0;\"&gt;&lt;a href=\"/wiki/London,_Ontario\" title=\"London, Ontario\"&gt;London&lt;/a&gt;&lt;/td&gt;, the html_text function would return London. population_text &lt;- html_text(population_nodes) head(population_text) ## [1] &quot;5,928,040&quot; &quot;London&quot; &quot;494,069\\n&quot; &quot;4,098,927&quot; ## [5] &quot;St. Catharines–Niagara&quot; &quot;406,074\\n&quot; Are we done? Not quite… If you look at the data closely you see that the data is not in an optimal format for data analysis. Both the city names and population are encoded as characters in a single vector instead of being in a data frame with one character column for city and one numeric column for population (think of how you would organize the data in a spreadsheet). Additionally, the populations contain commas (not useful for programmatically dealing with numbers), and some even contain a line break character at the end (\\n). Next chapter we will learn more about data wrangling using R so that we can easily clean up this data with a few lines of code. 3.9 Additional readings/resources Data import chapter from R for Data Science by Garrett Grolemund &amp; Hadley Wickham "],
["wrangling.html", "Chapter 4 Cleaning and wrangling data 4.1 Overview 4.2 Chapter learning objectives 4.3 Vectors and Data frames 4.4 Tidy Data 4.5 Combining functions using the pipe operator, %&gt;%: 4.6 Iterating over data with group_by + summarize 4.7 Additional reading on the dplyr functions 4.8 Using purrr’s map* functions to iterate 4.9 Additional readings/resources", " Chapter 4 Cleaning and wrangling data 4.1 Overview This chapter will be centered around tools for cleaning and wrangling data that move data from its raw format into a format that is suitable for data analysis. They will be presented in the context of a real world data science application, providing more practice working through a whole case study. 4.2 Chapter learning objectives By the end of the chapter, students will be able to: define the term “tidy data” discuss the advantages and disadvantages from storing data in a tidy data format recall and use the following tidyverse functions and operators for their intended data wrangling tasks: select filter %&gt;% map mutate summarize group_by gather separate %in% 4.3 Vectors and Data frames At this point, we know how to load data into R from various file formats. Once loaded into R, all the tools we have learned about for reading data into R represent the data as a data frame. So now we will spend some time learning more about data frames in R, such that we have a better understanding of how we can use and manipulate these objects. 4.3.1 What is a data frame? Let’s first start by defining exactly what a data frame is. From a data perspective, it is a rectangle where the rows are the observations: and the columns are the variables: From a computer programming perspective, in R, a data frame is a special subtype of a list object whose elements (columns) are vectors. For example, the data frame below has 3 elements that are vectors whose names are state, year and population. 4.3.2 What is a vector? In R, vectors are objects that can contain 1 or more elements. The vector elements are ordered, and they must all be of the same type. Common example types of vectors are character (e.g., letter or words), numeric (whole numbers and fractions) and logical (e.g., TRUE or FALSE). In the vector shown below, the elements are of numeric type: 4.3.3 How are vectors different from a list? Lists are also objects in R that have multiple elements. Vectors and lists differ by the requirement of element type consistency. All elements within a single vector must be of the same type (e.g., all elements are numbers), whereas elements within a single list can be of different types (e.g., characters, numbers, logicals and even other lists can be elements in the same list). 4.3.4 What does this have to do with data frames? As mentioned earlier, a data frame is really a special type of list where the elements can only be vectors. Representing data with such an object enables us to easily work with our data in a rectangular/spreadsheet like manner, and to have columns/vectors of different characteristics associated/linked in one object. This is similar to a table in a spreadsheet or a database. 4.4 Tidy Data There are many ways spreadsheet-like dataset can be organized. In this chapter we are going to focus on the tidy data format of organization, and how to make your raw (and likely messy) data tidy. This is because a variety of tools we would like to be able to use in R are designed to work most effectively (and efficiently) with tidy data. 4.4.1 What is tidy data? Tidy data satisfy the following three criteria: each row is a single observation, each column is a single variable, and each value is a single cell (i.e., its row and column position in the data frame is not shared with another value) image source: R for Data Science by Garrett Grolemund &amp; Hadley Wickham Definitions to know: observation - all of the quantities or a qualities we collect from a given entity/object variable - any characteristic, number, or quantity that can be measured or collected value - a single collected quantity or a quality from a given entity/object 4.4.2 Why is tidy data important in R? First, one of the most popular plotting toolsets in R, the ggplot2 library, expects the data to be in a tidy format. Second, most statistical analysis functions expect data in tidy format. Given that both of these tasks are central in virtually any data analysis project, it is well worth spending the time to get your data into a tidy format up front. Luckily there are many well designed tidyverse data cleaning/wrangling tools to help you easily tidy your data. Let’s explore them now! 4.4.3 Going from wide to long (or tidy!) using gather One common thing that often has to be done to get data into a tidy format is to combine columns that are really part the same variable but currently stored in separate columns. To do this we can use the function gather. gather acts to combine columns, and thus makes the data frame narrower. Data is often stored in a wider, not tidy, format because this format is often more intuitive for human readability and understanding, and humans create data sets. An example of this is shown below: library(tidyverse) hist_vote_wide &lt;- read_csv(&quot;data/historical_vote_wide.csv&quot;) hist_vote_wide ## # A tibble: 10 x 3 ## election_year winner runnerup ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2016 Donald Trump Hillary Clinton ## 2 2012 Barack Obama Mitt Romney ## 3 2008 Barack Obama John McCain ## 4 2004 George Bush John Kerry ## 5 2000 George Bush Al Gore ## 6 1996 Bill Clinton Bob Dole ## 7 1992 Bill Clinton George Bush ## 8 1988 George Bush Michael Dukakis ## 9 1984 Ronald Reagan Walter Mondale ## 10 1980 Ronald Reagan Jimmy Carter What is wrong with our untidy format above? From a data analysis perspective, this format is not ideal because in this format the outcome of the variable “result” (winner or runner up) is stored as column names and not easily accessible for the data analysis functions we will want to apply to our data set. Additionally, the values of the “candidate” variable are spread across two columns and will require some sort of binding or joining to get them into one single column to allow us to do our desired visualization and statistical tasks later on. To accomplish this data tranformation we will use the tidyverse function gather. To use gather we need to specify: the dataset the key: the name of a new column that will be created, whose values will come from the names of the columns that we want to combine (the result argument) the value: the name of a new column that will be created, whose values will come from the values of the columns we want to combine (the value argument) the names of the columns that we want to combine (we list these after specifying the key and value, and separate the column names with commas) For the above example, we use gather to combine the winner and runnerup columns into a single column called candidate, and create a column called result that contains the outcome of the election for each candidate: hist_vote_tidy &lt;- gather(hist_vote_wide, key = result, value = candidate, winner, runnerup) hist_vote_tidy ## # A tibble: 20 x 3 ## election_year result candidate ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2016 winner Donald Trump ## 2 2012 winner Barack Obama ## 3 2008 winner Barack Obama ## 4 2004 winner George Bush ## 5 2000 winner George Bush ## 6 1996 winner Bill Clinton ## 7 1992 winner Bill Clinton ## 8 1988 winner George Bush ## 9 1984 winner Ronald Reagan ## 10 1980 winner Ronald Reagan ## 11 2016 runnerup Hillary Clinton ## 12 2012 runnerup Mitt Romney ## 13 2008 runnerup John McCain ## 14 2004 runnerup John Kerry ## 15 2000 runnerup Al Gore ## 16 1996 runnerup Bob Dole ## 17 1992 runnerup George Bush ## 18 1988 runnerup Michael Dukakis ## 19 1984 runnerup Walter Mondale ## 20 1980 runnerup Jimmy Carter Splitting code across lines: In the code above, the call to the gather function is split across several lines. This is allowed and encouraged when programming in R when your code line gets too long to read clearly. When doing this, it is important to end the line with a comma , so that R knows the function should continue to the next line.* The data above is now tidy because all 3 criteria for tidy data have now been met: All the variables (candidate and result) are now their own columns in the data frame. Each observation, i.e., each candidate’s name, result, and candidacy year, are in a single row. Each value is a single cell, i.e., its row, column position in the data frame is not shared with another value. 4.4.4 Using separate to deal with multiple delimiters As discussed above, data are also not considered tidy when multiple values are stored in the same cell. In addition to the previous untidiness we addressed in the earlier version of this data set, the one we show below is even messier: the winner and runnerup columns contain both the candidate’s name as well as their political party. To make this messy data tidy we’ll have to fix both of these issues. hist_vote_party &lt;- read_csv(&quot;data/historical_vote_messy.csv&quot;) hist_vote_party ## # A tibble: 10 x 3 ## election_year winner runnerup ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2016 Donald Trump/Rep Hillary Clinton/Dem ## 2 2012 Barack Obama/Dem Mitt Romney/Rep ## 3 2008 Barack Obama/Dem John McCain/Rep ## 4 2004 George W Bush/Rep John Kerry/Dem ## 5 2000 George W Bush/Rep Al Gore/Dem ## 6 1996 Bill Clinton/Dem Bob Dole/Rep ## 7 1992 Bill Clinton/Dem George HW Bush/Rep ## 8 1988 George HW Bush/Rep Michael Dukakis/Dem ## 9 1984 Ronald Reagan/Rep Walter Mondale/Dem ## 10 1980 Ronald Reagan/Rep Jimmy Carter/Dem First we’ll use gather to create the result and candidate columns, as we did previously: hist_vote_party_gathered &lt;- gather(hist_vote_party, key = result, value = candidate, winner, runnerup) hist_vote_party_gathered ## # A tibble: 20 x 3 ## election_year result candidate ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2016 winner Donald Trump/Rep ## 2 2012 winner Barack Obama/Dem ## 3 2008 winner Barack Obama/Dem ## 4 2004 winner George W Bush/Rep ## 5 2000 winner George W Bush/Rep ## 6 1996 winner Bill Clinton/Dem ## 7 1992 winner Bill Clinton/Dem ## 8 1988 winner George HW Bush/Rep ## 9 1984 winner Ronald Reagan/Rep ## 10 1980 winner Ronald Reagan/Rep ## 11 2016 runnerup Hillary Clinton/Dem ## 12 2012 runnerup Mitt Romney/Rep ## 13 2008 runnerup John McCain/Rep ## 14 2004 runnerup John Kerry/Dem ## 15 2000 runnerup Al Gore/Dem ## 16 1996 runnerup Bob Dole/Rep ## 17 1992 runnerup George HW Bush/Rep ## 18 1988 runnerup Michael Dukakis/Dem ## 19 1984 runnerup Walter Mondale/Dem ## 20 1980 runnerup Jimmy Carter/Dem Then we’ll use separate to split the candidate column into two columns, one that contains only the candidate’s name (“candidate”), and one that contains a short identifier for which political party the candidate belonged to (“party”): hist_vote_party_tidy &lt;- separate(hist_vote_party_gathered, col = candidate, into = c(&quot;candidate&quot;, &quot;party&quot;), sep = &quot;/&quot;) hist_vote_party_tidy ## # A tibble: 20 x 4 ## election_year result candidate party ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2016 winner Donald Trump Rep ## 2 2012 winner Barack Obama Dem ## 3 2008 winner Barack Obama Dem ## 4 2004 winner George W Bush Rep ## 5 2000 winner George W Bush Rep ## 6 1996 winner Bill Clinton Dem ## 7 1992 winner Bill Clinton Dem ## 8 1988 winner George HW Bush Rep ## 9 1984 winner Ronald Reagan Rep ## 10 1980 winner Ronald Reagan Rep ## 11 2016 runnerup Hillary Clinton Dem ## 12 2012 runnerup Mitt Romney Rep ## 13 2008 runnerup John McCain Rep ## 14 2004 runnerup John Kerry Dem ## 15 2000 runnerup Al Gore Dem ## 16 1996 runnerup Bob Dole Rep ## 17 1992 runnerup George HW Bush Rep ## 18 1988 runnerup Michael Dukakis Dem ## 19 1984 runnerup Walter Mondale Dem ## 20 1980 runnerup Jimmy Carter Dem Is this data now tidy? Well, if we recall the 3 criteria for tidy data: each row is a single observation, each column is a single variable, and each value is a single cell. We can see that this data now satifies all 3 criteria, making it easier to analyze. For example, we could visualize the number of winning candidates for each party over this time span: ggplot(hist_vote_party_tidy, aes(x = result, fill = party)) + geom_bar() + scale_fill_manual(values=c(&quot;blue&quot;, &quot;red&quot;)) + xlab(&quot;US Presidential election result&quot;) + ylab(&quot;Number of US Presidential candidates&quot;) + ggtitle(&quot;US Presidential candidates (1980 - 2016)&quot;) From this visualization, we can see that between 1980 - 2016 (inclusive) the Republican party has won more US Presidential elections than the Democratic party. 4.4.5 Notes on defining tidy data Is there only one shape for tidy data for a given data set? Not necessarily, it depends on the statistical question you are asking and what the variables are for that question. For tidy data, each variable should be its own column. So just as its important to match your statistical question with the appropriate data analysis tool (classification, clustering, hypothesis testing, etc). It’s important to match your statistical question with the appropriate variables and ensure they are each represented as individual columns to make the data tidy. 4.5 Combining functions using the pipe operator, %&gt;%: In R, we often have to call multiple functions in a sequence to process a data frame. The basic ways of doing this can become quickly unreadable if there are many steps. For example, suppose we need to perform three operations on a data frame data: add a new column new_col that is double another old_col filter for rows where another column, other_col, is more than 5, and select only the new column new_col for those rows. One way of doing is to just write multiple lines of code, storing temporary objects as you go: output_1 &lt;- mutate(data, new_col = old_col*2) output_2 &lt;- filter(output_1, other_col &gt; 5) output &lt;- select(output_2, new_col) This is difficult to understand for multiple reasons. The reader may be tricked into thinking the named output_1 and output_2 objects are important for some reason, while they are just temporary intermediate computations. Further, the reader has to look through and find where output_1 and output_2 are used in each subsequent line. Another option for doing this would be to compose the functions: output &lt;- select(filter(mutate(data, new_col = old_col*2), other_col &gt; 5), new_col) Code like this can also be difficult to understand. Functions compose (reading from left to right) in the opposite order in which they are computed by R (above, mutate happens first, then filter, then select). It is also just a really long line of code to read in one go. The pipe operator %&gt;% solves this problem, resulting in cleaner and easier-to-follow code. The below accomplishes the same thing as the previous two code blocks: output &lt;- data %&gt;% mutate(new_col = old_col*2) %&gt;% filter(other_col &gt; 5) %&gt;% select(new_col) You can think of the pipe as a physical pipe. It takes the output from the function on the left-hand side of the pipe, and passes it as the first argument to the function on the right-hand side of the pipe. Note here that we have again split the code across multiple lines for readability; R is fine with this, since it knows that a line ending in a pipe %&gt;% is continued on the next line. Next, let’s learn about the details of using the pipe, and look at some examples of how to use it in data analysis. 4.5.1 Using %&gt;% to combine filter and select Recall the US state-level property, income, population, and voting data that we explored in chapter 1: us_data &lt;- read_csv(&quot;data/state_property_vote.csv&quot;) us_data ## # A tibble: 52 x 6 ## state med_income med_prop_val population mean_commute_minutes party ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 AK 64222 197300 733375 10.5 Republican ## 2 AL 36924 94800 4830620 25.3 Republican ## 3 AR 35833 83300 2958208 22.4 Republican ## 4 AZ 44748 128700 6641928 20.6 Republican ## 5 CA 53075 252100 38421464 23.4 Democrat ## 6 CO 48098 198900 5278906 19.5 Democrat ## 7 CT 69228 246450 3593222 24.3 Democrat ## 8 DC 70848 475800 647484 28.3 Democrat ## 9 DE 54976 228500 926454 24.5 Democrat ## 10 FL 43355 125600 19645772 24.8 Republican ## # … with 42 more rows Suppose we want to create a subset of the data with only the values for median income and median property value for the state of California (“CA”). To do this, we can use the functions filter and select. First we use filter to create a data frame called ca_prop_data that contains only values for the state of California. We then use select on this data frame to keep only the median income and median property value variables: ca_prop_data &lt;- filter(us_data, state == &quot;CA&quot;) ca_inc_prop &lt;- select(ca_prop_data, med_income, med_prop_val) ca_inc_prop ## # A tibble: 1 x 2 ## med_income med_prop_val ## &lt;dbl&gt; &lt;dbl&gt; ## 1 53075 252100 Although this is valid code, there is a more readable approach we could take by using the pipe, %&gt;%. With the pipe, we do not need to create an intermediate object to store the output from filter. Instead we can directly send the output of filter to the input of select: ca_inc_prop &lt;- filter(us_data, state == &quot;CA&quot;) %&gt;% select(med_income, med_prop_val) ca_inc_prop ## # A tibble: 1 x 2 ## med_income med_prop_val ## &lt;dbl&gt; &lt;dbl&gt; ## 1 53075 252100 But wait - why does our select function call look different in these two examples? When you use the pipe, the output of the function on the left is automatically provided as the first argument for the function on the right, and thus you do not specify that argument in that function call. In the code above, the first argument of select is the data frame we are select-ing from, which is provided by the output of filter. As you can see, both of these approaches give us the same output but the second approach is more clear and readable. 4.5.2 Using %&gt;% with more than two functions The %&gt;% can be used with any function in R. Additionally, we can pipe together more than two functions. For example, we can pipe together three functions to order the states by commute time for states whose population is less than 1 million people: small_state_commutes &lt;- filter(us_data, population &lt; 1000000) %&gt;% select(state, mean_commute_minutes) %&gt;% arrange(mean_commute_minutes) small_state_commutes ## # A tibble: 7 x 2 ## state mean_commute_minutes ## &lt;chr&gt; &lt;dbl&gt; ## 1 AK 10.5 ## 2 SD 15.1 ## 3 ND 16.3 ## 4 WY 18.7 ## 5 VT 22.3 ## 6 DE 24.5 ## 7 DC 28.3 Note:: arrange is a function that takes the name of a data frame and one or more column(s), and returns a data frame where the rows are ordered by those columns in ascending order. Here we used only one column for sorting (mean_commute_minutes), but more than one can also be used. To do this, list additional columns separated by commas. The order they are listed in indicates the order in which they will be used for sorting. This is much like how an English dictionary sorts words: first by the first letter, then by the second letter, and so on. Another Note: You might also have noticed that we split the function calls across lines after the pipe, similar as to when we did this earlier in the chapter for long function calls. Again this is allowed and recommeded, especially when the piped function calls would create a long line of code. Doing this makes your code more readable. When you do this it is important to end each line with the pipe operator %&gt;% to tell R that your code is continuing onto the next line. 4.6 Iterating over data with group_by + summarize 4.6.1 Calculating summary statistics: As a part of many data analyses, we need to calculate a summary value for the data (a summary statistic). A useful dplyr function for doing this is summarize. Examples of summary statistics we might want to calculate are the number of observations, the average/mean value for a column, the minimum value for a column, etc. Below we show how to use the summarize function to calculate the minimum, maximum and mean commute time for all US states: us_commute_time_summary &lt;- summarize(us_data, min_mean_commute = min(mean_commute_minutes), max_mean_commute = max(mean_commute_minutes), mean_mean_commute = mean(mean_commute_minutes)) us_commute_time_summary ## # A tibble: 1 x 3 ## min_mean_commute max_mean_commute mean_mean_commute ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 10.5 29.0 22.2 4.6.2 Calculating group summary statistics: A common pairing with summarize is group_by. Pairing these functions together can let you summarize values for subgroups within a data set. For example, here we can use group_by to group the states based on which party they voted for in the US election, and then calculate the minimum, maximum and mean commute time for each of the groups. The group_by function takes at least two arguments. The first is the data frame that will be grouped, and the second and onwards are columns to use in the grouping. Here we use only one column for grouping (party), but more than one can also be used. To do this, list additional columns separated by commas. us_commute_time_summary_by_party &lt;- group_by(us_data, party) %&gt;% summarize(min_mean_commute = min(mean_commute_minutes), max_mean_commute = max(mean_commute_minutes), mean_mean_commute = mean(mean_commute_minutes)) ## `summarise()` ungrouping output (override with `.groups` argument) us_commute_time_summary_by_party ## # A tibble: 3 x 4 ## party min_mean_commute max_mean_commute mean_mean_commute ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Democrat 18.5 28.8 23.2 ## 2 Not Applicable 29.0 29.0 29.0 ## 3 Republican 10.5 27.1 21.4 4.7 Additional reading on the dplyr functions We haven’t explicitly said this yet, but the tidyverse is actually a meta R package: it installs a collection of R packages that all follow the tidy data philosophy we discussed above. One of the tidyverse packages is dplyr - a data wrangling workhorse. You have already met 6 of the dplyr function (select, filter, mutate, arrange, summarize, and group_by). To learn more about those six and meet a few more useful functions, read the post at this link. 4.8 Using purrr’s map* functions to iterate Where should you turn when you discover the next step in your data wrangling/cleaning process requires you to apply a function to each column in a data frame? For example, if you wanted to know the maximum value of each column in a data frame? Well you could use summarize as discussed above, but this becomes inconvenient when you have many columns, as summarize requires you to type out a column name and a data tranformation for each summary statistic that you want to calculate. In cases like this, where you want to apply the same data transformation to all columns, it is more efficient to use purrr’s map function to apply it to each column. For example, let’s find the maximum value of each column of the mtcars data frame (a built-in data set that comes with R) by using map with the max function. First, let’s peak at the data to familiarize ourselves with it: head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 Next, we use map to apply the max function to each column. map takes two arguments, an object (a vector, data frame or list) that you want to apply the function to, and the function that you would like to apply. Here our arguments will be mtcars and max: max_of_columns &lt;- map(mtcars, max) max_of_columns ## $mpg ## [1] 33.9 ## ## $cyl ## [1] 8 ## ## $disp ## [1] 472 ## ## $hp ## [1] 335 ## ## $drat ## [1] 4.93 ## ## $wt ## [1] 5.424 ## ## $qsec ## [1] 22.9 ## ## $vs ## [1] 1 ## ## $am ## [1] 1 ## ## $gear ## [1] 5 ## ## $carb ## [1] 8 Note: purrr is part of the tidyverse, and so like the dplyr and ggplot functions, once we call library(tidyverse) we do not need to separately load the purrr package. Our output looks a bit weird…we passed in a data frame, but our output doesn’t look like a data frame. As it so happens, it is not a data frame, but rather a plain vanilla list: typeof(max_of_columns) ## [1] &quot;list&quot; So what do we do? Should we convert this to a data frame? We could, but a simpler alternative is to just use a different map_* function from the purrr package. There are quite a few to choose from, they all work similarly and their name refects the type of output you want from the mapping operation: map function Output map() list map_lgl() logical vector map_int() integer vector map_dbl() double vector map_chr() character vector map_df() data frame Let’s get the column maximum’s again, but this time use the map_df function to return the output as a data frame: max_of_columns &lt;- map_df(mtcars, max) max_of_columns ## # A tibble: 1 x 11 ## mpg cyl disp hp drat wt qsec vs am gear carb ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 33.9 8 472 335 4.93 5.42 22.9 1 1 5 8 Which map_* function you choose depends on what you want to do with the output; you don’t always have to pick map_df! What if you need to add other arguments to the functions you want to map? For example, what if there were NA values in our columns that we wanted to know the maximum of? Well then we also need to add the argument na.rm = TRUE to the max function so that we get a more useful value than NA returned (remember that is what happens with many of the built-in R statistical functions when NA’s are present…). What we need to do in that case is do what is called “creating an anonymous function” within the map_df function. We do that in the place where we previously specified our max function. Here we will put the two calls to map_df right after each other so you can see the difference: # no additional arguments to the max function map_df(mtcars, max) versus # adding the na.rm = TRUE argument to the max function map_df(mtcars, function(df) max(df, na.rm = TRUE)) You can see that’s quite a bit of extra typing… So the creators of purrr have made a shortcut for this because it is so commonly done. In the shortcut we replace function(VARIABLE) with a ~ and replace the VARIABLE in the function call with a ., see the example below: # adding the na.rm = TRUE argument to the max function using the shortcut map_df(mtcars, ~ max(., na.rm = TRUE)) 4.8.1 A bit more about the map_* functions The map_* functions are generally quite useful for solving problems involving iteration/repetition. Additionally, their use is not limited to columns of a data frame; map_* functions can be used to apply functions to elements of a vector or list, and even to lists of data frames, or nested data frames. 4.9 Additional readings/resources Grolemund &amp; Wickham’s R for Data Science has a number of useful sections that provide additional information: Data transformation Tidy data The map_* functions "],
["viz.html", "Chapter 5 Effective data visualization 5.1 Overview 5.2 Chapter learning objectives 5.3 Choosing the visualization 5.4 Refining the visualization 5.5 Creating visualizations with ggplot2 5.6 Explaining the visualization 5.7 Saving the visualization", " Chapter 5 Effective data visualization 5.1 Overview This chapter will introduce concepts and tools relating to data visualization beyond what we have seen and practiced so far. We will focus on guiding principles for effective data visualization and explaining visualizations independent of any particular tool or programming language. In the process, we will cover some specifics of creating visualizations (scatter plots, bar charts, line graphs, and histograms) for data using R. There are external references that contain a wealth of additional information on the topic of data visualization: Professor Claus Wilke’s Fundamentals of Data Visualization has more details on general principles of effective visualizations Grolemund &amp; Wickham’s R for Data Science chapter on creating visualizations using ggplot2 has a deeper introduction into the syntax and grammar of plotting with ggplot2 specifically the ggplot2 reference has a useful list of useful ggplot2 functions 5.2 Chapter learning objectives Describe when to use the following kinds of visualizations: scatter plots line plots bar plots histogram plots Given a data set and a question, select from the above plot types to create a visualization that best answers the question Given a visualization and a question, evaluate the effectiveness of the visualization and suggest improvements to better answer the question Identify rules of thumb for creating effective visualizations Define the three key aspects of ggplot objects: aesthetic mappings geometric objects scales Use the ggplot2 library in R to create and refine the above visualizations using: geometric objects: geom_point, geom_line, geom_histogram, geom_bar, geom_vline, geom_hline scales: scale_x_continuous, scale_y_continuous aesthetic mappings: x, y, fill, colour, shape labelling: xlab, ylab, labs font control and legend positioning: theme flipping axes: coord_flip subplots: facet_grid Describe the difference in raster and vector output formats Use ggsave to save visualizations in .png and .svg format 5.3 Choosing the visualization Ask a question, and answer it The purpose of a visualization is to answer a question about a data set of interest. So naturally, the first thing to do before creating a visualization is to formulate the question about the data that you are trying to answer. A good visualization will answer your question in a clear way without distraction; a great visualization will suggest even what the question was itself without additional explanation. Imagine your visualization as part of a poster presentation for your project; even if you aren’t standing at the poster explaining things, an effective visualization will be able to convey your message to the audience. Recall the different types of data analysis question from the very first chapter of this book. With the visualizations we will cover in this chapter, we will be able to answer only descriptive and exploratory questions. Be careful not to try to answer any predictive, inferential, causal or mechanistic questions, as we have not learned the tools necessary to do that properly just yet. As with most coding tasks, it is totally fine (and quite common) to make mistakes and iterate a few times before you find the right visualization for your data and question. There are many different kinds of plotting graphic available to use. For the kinds we will introduce in this course, the general rules of thumb are: line plots visualize trends with respect to an independent, ordered quantity (e.g. time) histograms visualize the distribution of one quantitative variable (i.e., all its possible values and how often they occur) scatter plots visualize the distribution / relationship of two quantitative variables bar plots visualize comparisons of amounts All types of visualization have their (mis)uses, but there are three kinds that are usually hard to understand or are easily replaced with an oft-better alternative. In particular you should avoid pie charts; it is usually better to use bars, as it is easier to compare bar heights than pie slice sizes. You should also not use 3-D visualizations, as they are typically hard to understand when converted to a static 2-D image format. Finally, do not use tables to make numerical comparisons; humans are much better at quickly processing visual information than text and math. Bar plots are again typically a better alternative. 5.4 Refining the visualization Convey the message, minimize noise Just being able to make a visualization in R with ggplot2 (or any other tool for that matter) doesn’t mean that it is effective at communicating your message to others. Once you have selected a broad type of visualization to use, you will have to refine it to suit your particular need. Some rules of thumb for doing this are listed below. They generally fall into two classes: you want to make your visualization convey your message, and you want to reduce visual noise as much as possible. Humans have limited cognitive ability to process information; both of these types of refinement aim to reduce the mental load on your audience when viewing your visualization, making it easier for them to quickly understand and remember your message. Convey the message Make sure the visualization answers the question you have asked in the simplest and most plain way possible. Use legends and labels so that your visualization is understandable without reading the surrounding text. Ensure the text, symbols, lines, etc. on your visualization are big enough to be easily read. Make sure the data are clearly visible; don’t hide the shape/distribution of the data behind other objects (e.g. a bar). Make sure to use colourschemes that are understandable by those with colourblindness (a surprisingly large fraction of the overall population). For example, colorbrewer.org and the RColorBrewer R library provide the ability to pick such colourschemes, and you can check your visualizations after you have created them by uploading to online tools such as the colour blindness simulator. Redundancy can be helpful; sometimes conveying the same message in multiple ways reinforces it for the audience. Minimize noise Use colours sparingly. Too many different colours can be distracting, create false patterns, and detract from the message. Be wary of overplotting. If your plot has too many dots or lines and it starts to look like a mess, then you need to do something different. Only make the plot area (where the dots, lines, bars are) as big as needed. Simple plots can be made small. Don’t adjust the axes to zoom in on small differences. If the difference is small, show that it’s small! 5.5 Creating visualizations with ggplot2 Build the visualization iteratively This section will cover examples of how to choose and refine a visualization given a data set and a question that you want to answer, and then how to create the visualization in R using ggplot2. To use the ggplot2 library, we need to load the tidyverse metapackage. library(tidyverse) 5.5.1 The Mauna Loa CO2 data set This data set contains the atmospheric concentration of carbon dioxide (CO2, in parts per million) at the Mauna Loa research station in Hawaii from the years 1959-1997. Question: Does the concentration of atmospheric CO2 change over time, and are there any interesting patterns to note? # mauna loa carbon dioxide data co2_df &lt;- read_csv(&quot;data/maunaloa.csv&quot;) head(co2_df) ## # A tibble: 6 x 2 ## concentration date ## &lt;dbl&gt; &lt;dbl&gt; ## 1 315. 1959 ## 2 316. 1959. ## 3 316. 1959. ## 4 318. 1959. ## 5 318. 1959. ## 6 318 1959. Since we are investigating a relationship between two variables (CO2 concentration and date), a scatter plot is a good place to start. Scatter plots show the data as individual points with x (horizonal axis) and y (vertical axis) coordinates. Here, we will use the date as the x coordinate and CO2 concentration as the y coordinate. When using the ggplot2 library, we create the plot object with the ggplot function; there are a few basic aspects of a plot that we need to specify: the data: the name of the dataframe object that we would like to visualize here, we specify the co2_df dataframe the aesthetic mapping: tells ggplot how the columns in the dataframe map to properties of the visualization to create an aesthetic mapping, we use the aes function here, we set the plot x axis to the date variable, and the plot y axis to the concentration variable the geometric object: specifies how the mapped data should be displayed to create a geometric object, we use a geom_* function (see the ggplot reference for a list of geometric objects) here, we use the geom_point function to visualize our data as a scatterplot There are many other possible arguments we could pass to the aesthetic mapping and geometric object to change how the plot looks. For the purposes of quickly testing things out to see what they look like, though, we can just go with the default settings: co2_scatter &lt;- ggplot(co2_df, aes(x = date, y = concentration)) + geom_point() co2_scatter Certainly the visualization shows a clear upward trend in the atmospheric concentration of CO2 over time. This plot answers the first part of our question in the affirmative, but that appears to be the only conclusion one can make from the scatter visualization. However, since time is an ordered quantity, we can try using a line plot instead using the geom_line function. Line plots require that the data are ordered by their x coordinate, and connect the sequence of x and y coordinates with line segments. Let’s again try this with just the default arguments: co2_line &lt;- ggplot(co2_df, aes(x = date, y = concentration)) + geom_line() co2_line Aha! There is another interesting phenomenon in the data: in addition to increasing over time, the concentration seems to oscillate as well. Given the visualization as it is now, it is still hard to tell how fast the oscillation is, but nevertheless, the line seems to be a better choice for answering the question than the scatter plot was. The comparison between these two visualizations illustrates a common issue with scatter plots: often the points are shown too close together or even on top of one another, muddling information that would otherwise be clear (overplotting). Now that we have settled on the rough details of the visualization, it is time to refine things. This plot is fairly straightforward, and there is not much visual noise to remove. But there are a few things we must do to improve clarity, such as adding informative axis labels and making the font a more readable size. In order to add axis labels we use the xlab and ylab functions. To change the font size we use the theme function with the text argument: co2_line &lt;- ggplot(co2_df, aes(x = date, y = concentration)) + geom_line() + xlab(&#39;Year&#39;) + ylab(&#39;Atmospheric CO2 (ppm)&#39;) + theme(text = element_text(size = 18)) co2_line Finally, let’s see if we can better understand the oscillation by changing the visualization a little bit. Note that it is totally fine to use a small number of visualizations to answer different aspects of the question you are trying to answer. We will accomplish this by using scales, another important feature of ggplot2 that allow us to easily transform the different variables and set limits. We scale the horizontal axis by using the scale_x_continuous function, and the vertical axis with the scale_y_continuous function. We can transform the axis by passing the trans argument, and set limits by passing the limits argument. In particular, here we will use the scale_x_continuous function with the limits argument to zoom in on just five years of data (say, 1990-1995): co2_line &lt;- ggplot(co2_df, aes(x = date, y = concentration)) + geom_line() + xlab(&#39;Year&#39;) + ylab(&#39;Atmospheric CO2 (ppm)&#39;) + scale_x_continuous(limits = c(1990, 1995)) + theme(text = element_text(size = 18)) co2_line Interesting! It seems that each year, the atmospheric CO2 increases until it reaches its peak somewhere around April, decreases until around late September, and finally increases again until the end of the year. In Hawaii, there are two seasons: summer from May through October, and winter from November through April. Therefore, the oscillating pattern in CO2 matches up fairly closely with the two seasons. 5.5.2 The island landmass data set This data set contains a list of Earth’s land masses as well as their area (in thousands of square miles). Question: Are the continents (North / South America, Africa, Europe, Asia, Australia, Antarctica) Earth’s 7 largest landmasses? If so, what are the next few largest landmasses after those? # islands data islands_df &lt;- read_csv(&quot;data/islands.csv&quot;) head(islands_df) ## # A tibble: 6 x 2 ## landmass size ## &lt;chr&gt; &lt;dbl&gt; ## 1 Africa 11506 ## 2 Antarctica 5500 ## 3 Asia 16988 ## 4 Australia 2968 ## 5 Axel Heiberg 16 ## 6 Baffin 184 Here, we have a list of Earth’s landmasses, and are trying to compare their sizes. The right type of visualization to answer this question is a bar plot, specified by the geom_bar function in ggplot2. However, by default, geom_bar sets the heights of bars to the number of times a value appears in a dataframe (its count); here we want to plot exactly the values in the dataframe, i.e., the landmass sizes. So we have to pass the stat = \"identity\" argument to geom_bar: islands_bar &lt;- ggplot(islands_df, aes(x = landmass, y = size)) + geom_bar(stat = &quot;identity&quot;) islands_bar Alright, not bad! This is definitely the right kind of visualization, as we can clearly see and compare sizes of landmasses. The major issues are that the sizes of the smaller landmasses are hard to distinguish, and that the names of the landmasses are obscuring each other as they have been squished into too little space. But remember that the question we asked was only about the largest landmasses; let’s make the plot a little bit clearer by keeping only the largest 12 landmasses. We do this using the top_n function. Then to help us make sure the labels have enough space, we’ll use horizontal bars instead of vertical ones. We do this using the coord_flip function, which swaps the x and y coordinate axes: islands_top12 &lt;- top_n(islands_df, 12, size) islands_bar &lt;- ggplot(islands_top12, aes(x = landmass, y = size)) + geom_bar(stat = &quot;identity&quot;) + coord_flip() islands_bar This is definitely clearer now, and allows us to answer our question (“are the top 7 largest landmasses continents?”) in the affirmative. But the question could be made clearer from the plot by organizing the bars not by alphabetical order but by size, and to colour them based on whether or not they are a continent. In order to do this, we use mutate to add a column to the data regarding whether or not the landmass is a continent: islands_top12 &lt;- top_n(islands_df, 12, size) continents &lt;- c(&#39;Africa&#39;, &#39;Antarctica&#39;, &#39;Asia&#39;, &#39;Australia&#39;, &#39;Europe&#39;, &#39;North America&#39;, &#39;South America&#39;) islands_ct &lt;- mutate(islands_top12, is_continent = ifelse(landmass %in% continents, &#39;Continent&#39;, &#39;Other&#39;)) head(islands_ct) ## # A tibble: 6 x 3 ## landmass size is_continent ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Africa 11506 Continent ## 2 Antarctica 5500 Continent ## 3 Asia 16988 Continent ## 4 Australia 2968 Continent ## 5 Baffin 184 Other ## 6 Borneo 280 Other In order to colour the bars, we add the fill argument to the aesthetic mapping. Then we use the reorder function in the aesthetic mapping to organize the landmasses by their size variable. Finally, we use the labs and theme functions to add labels, change the font size, and position the legend: islands_bar &lt;- ggplot(islands_ct, aes(x = reorder(landmass, size), y = size, fill = is_continent)) + geom_bar(stat=&quot;identity&quot;) + labs(x = &#39;Landmass&#39;, y = &#39;Size (1000 square mi)&#39;, fill = &#39;Type&#39;) + coord_flip() + theme(text = element_text(size = 18), legend.position = c(0.75, 0.45)) islands_bar This is now a very effective visualization for answering our original questions. Landmasses are organized by their size, and continents are coloured differently than other landmasses, making it quite clear that continents are the largest 7 landmasses. 5.5.3 The Old Faithful eruption / waiting time data set This data set contains measurements of the waiting time between eruptions and the subsequent eruption duration (in minutes). Question: Is there a relationship between the waiting time before an eruption to the duration of the eruption? # old faithful eruption time / wait time data head(faithful) ## eruptions waiting ## 1 3.600 79 ## 2 1.800 54 ## 3 3.333 74 ## 4 2.283 62 ## 5 4.533 85 ## 6 2.883 55 Here again we are investigating the relationship between two quantitative variables (waiting time and eruption time). But if you look at the output of the head function, you’ll notice that neither of the columns are ordered. So in this case, let’s start again with a scatter plot: faithful_scatter &lt;- ggplot(faithful, aes(x = waiting, y = eruptions)) + geom_point() faithful_scatter We can see that the data tend to fall into two groups: one with a short waiting and eruption times, and one with long waiting and eruption times. Note that in this case, there is no overplotting: the points are generally nicely visually separated, and the pattern they form is clear. In order to refine the visualization, we need only to add axis labels and make the font more readable: faithful_scatter &lt;- ggplot(faithful, aes(x = waiting, y = eruptions)) + geom_point() + labs(x = &#39;Waiting Time (mins)&#39;, y = &#39;Eruption Duration (mins)&#39;) + theme(text = element_text(size = 18)) faithful_scatter 5.5.4 The Michelson speed of light data set This data set contains measurements of the speed of light (in kilometres per second with 299,000 subtracted) from the year 1879 for 5 experiments, each with 20 consecutive runs. Question: Given what we know now about the speed of light (299,792.458 kilometres per second), how accurate were each of the experiments? # michelson morley experimental data head(morley) ## Expt Run Speed ## 001 1 1 850 ## 002 1 2 740 ## 003 1 3 900 ## 004 1 4 1070 ## 005 1 5 930 ## 006 1 6 850 In this experimental data, Michelson was trying to measure just a single quantitative number (the speed of light). The data set contains many measurements of this single quantity. To tell how accurate the experiments were, we need to visualize the distribution of the measurements (i.e., all their possible values and how often each occurs). We can do this using a histogram. A histogram helps us visualize how a particular variable is distributed in a data set by separating the data into bins, and then using vertical bars to show how many data points fell in each bin. To create a histogram in ggplot2 we will use the geom_histogram geometric object, setting the x axis to the Speed measurement variable; and as we did before, let’s use the default arguments just to see how things look: morley_hist &lt;- ggplot(morley, aes(x = Speed)) + geom_histogram() morley_hist This is a great start. However, we cannot tell how accurate the measurements are using this visualization unless we can see what the true value is. In order to visualize the true speed of light, we will add a vertical line with the geom_vline function, setting the xintercept argument to the true value. There is a similar function, geom_hline, that is used for plotting horizontal lines. Note that vertical lines are used to denote quantities on the horizontal axis, while horizontal lines are used to denote quantities on the vertical axis. morley_hist &lt;- ggplot(morley, aes(x = Speed)) + geom_histogram() + geom_vline(xintercept = 792.458, linetype = &quot;dashed&quot;, size = 1.0) morley_hist We also still cannot tell which experiments (denoted in the Expt column) led to which measurements; perhaps some experiments were more accurate than others. To fully answer our question, we need to separate the measurements from each other visually. We can try to do this using a coloured histogram, where counts from different experiments are stacked on top of each other in different colours. We create a histogram coloured by the Expt variable by adding it to the fill aesthetic mapping. We make sure the different colours can be seen (despite them all sitting on top of each other) by setting the alpha argument in geom_histogram to 0.5 to make the bars slightly translucent: morley_hist &lt;- ggplot(morley, aes(x = Speed, fill = factor(Expt))) + geom_histogram(position = &quot;identity&quot;, alpha = 0.5) + geom_vline(xintercept = 792.458, linetype = &quot;dashed&quot;, size = 1.0) morley_hist Unfortunately, the attempt to separate out the experiment number visually has created a bit of a mess. All of the colours are blending together, and although it is possible to derive some insight from this (e.g., experiments 1 and 3 had some of the most incorrect measurements), it isn’t the clearest way to convey our message and answer the question. Let’s try a different strategy of creating multiple separate histograms on top of one another. In order to create a plot in ggplot2 that has multiple subplots arranged in a grid, we use the facet_grid function. The argument to facet_grid specifies the variable(s) used to split the plot into subplots. It has the syntax vertical_variable ~ horizontal_variable, where veritcal_variable is used to split the plot vertically, horizontal_variable is used to split horizontally, and . is used if there should be no split along that axis. In our case we only want to split vertically along the Expt variable, so we use Expt ~ . as the argument to facet_grid. morley_hist &lt;- ggplot(morley, aes(x = Speed, fill = factor(Expt))) + geom_histogram(position = &quot;identity&quot;) + facet_grid(Expt ~ .) + geom_vline(xintercept = 792.458, linetype = &quot;dashed&quot;, size = 1.0) morley_hist The visualization now makes it quite clear how accurate the different experiments were with respect to one another. There are two finishing touches to make this visualization even clearer. First and foremost, we need to add informative axis labels using the labs function, and increase the font size to make it readable using the theme function. Second, and perhaps more subtly, even though it is easy to compare the experiments on this plot to one another, it is hard to get a sense for just how accurate all the experiments were overall. For example, how accurate is the value 800 on the plot, relative to the true speed of light? To answer this question we’ll use the mutate function to transform our data into a relative measure of accuracy rather than absolute measurements: morley_rel &lt;- mutate(morley, relative_accuracy = 100*( (299000 + Speed) - 299792.458 ) / (299792.458)) morley_hist &lt;- ggplot(morley_rel, aes(x = relative_accuracy, fill = factor(Expt))) + geom_histogram(position = &quot;identity&quot;) + facet_grid(Expt ~ .) + geom_vline(xintercept = 0, linetype = &quot;dashed&quot;, size = 1.0) + labs(x = &#39;Relative Accuracy (%)&#39;, y = &#39;# Measurements&#39;, fill = &#39;Experiment ID&#39;) + theme(text = element_text(size = 18)) morley_hist Wow, impressive! These measurements of the speed of light from 1879 had errors around 0.05% of the true speed. This shows you that even though experiments 2 and 5 were perhaps the most accurate, all of the experiments did quite an admirable job given the technology available at the time period. 5.6 Explaining the visualization Tell a story Typically, your visualization will not be shown completely on its own, but rather it will be part of a larger presentation. Further, visualizations can provide supporting information for any part of a presentation, from opening to conclusion. For example, you could use an exploratory visualization in the opening of the presentation to motivate your choice of a more detailed data analysis / model, a visualization of the results of your analysis to show what your analysis has uncovered, or even one at the end of a presentation to help suggest directions for future work. Regardless of where it appears, a good way to discuss your visualization is as a story: Establish the setting and scope, and motivate why you did what you did. Pose the question that your visualization answers. Justify why the question is important to answer. Answer the question using your visualization. Make sure you describe all aspects of the visualization (including describing the axes). But you can emphasize different aspects based on what is important to answering your question: trends (lines): Does a line describe the trend well? If so, the trend is linear, and if not, the trend is nonlinear. Is the trend increasing, decreasing, or neither? Is there a periodic oscillation (wiggle) in the trend? Is the trend noisy (does the line “jump around” a lot) or smooth? distributions (scatters, histograms): How spread out are the data? Where are they centered, roughly? Are there any obvious “clusters” or “subgroups”, which would be visible as multiple bumps in the histogram? distributions of two variables (scatters): is there a clear / strong relationship between the variables (points fall in a distinct pattern), a weak one (points fall in a pattern but there is some noise), or no discernible relationship (the data are too noisy to make any conclusion)? amounts (bars): How large are the bars relative to one another? Are there patterns in different groups of bars? Summarize your findings, and use them to motivate whatever you will discuss next. Below are two examples of how might one take these four steps in describing the example visualizations that appeared earlier in this chapter. Each of the steps is denoted by its numeral in parentheses, e.g. (3). Mauna Loa Atmospheric CO2 Measurements: (1) Many current forms of energy generation and conversion—from automotive engines to natural gas power plants—rely on burning fossil fuels and produce greenhouse gases, typically primarily carbon dioxide (CO2), as a byproduct. Too much of these gases in the Earth’s atmosphere will cause it to trap more heat from the sun, leading to global warming. (2) In order to assess how quickly the atmospheric concentration of CO2 is increasing over time, we (3) used a data set from the Mauna Loa observatory from Hawaii, consisting of CO2 measurements from 1959 to the present. We plotted the measured concentration of CO2 (on the vertical axis) over time (on the horizontal axis). From this plot you can see a clear, increasing, and generally linear trend over time. There is also a periodic oscillation that occurs once per year and aligns with Hawaii’s seasons, with an amplitude that is small relative to the growth in the overall trend. This shows that atmospheric CO2 is clearly increasing over time, and (4) it is perhaps worth investigating more into the causes. Michelson Light Speed Experiments: (1) Our modern understanding of the physics of light has advanced significantly from the late 1800s when experiments of Michelson and Morley first demonstrated that it had a finite speed. We now know based on modern experiments that it moves at roughly 299792.458 kilometres per second. (2) But how accurately were we first able to measure this fundamental physical constant, and did certain experiments produce more accurate results than others? (3) To better understand this we plotted data from 5 experiments by Michelson in 1879, each with 20 trials, as histograms stacked on top of one another. The horizontal axis shows the accuracy of the measurements relative to the true speed of light as we know it today, expressed as a percentage. From this visualization you can see that most results had relative errors of at most 0.05%. You can also see that experiments 1 and 3 had measurements that were the farthest from the true value, and experiment 5 tended to provide the most consistently accurate result. (4) It would be worth further investigation into the differences between these experiments to see why they produced different results. 5.7 Saving the visualization Choose the right output format for your needs Just as there are many ways to store data sets, there are many ways to store visualizations and images. Which one you choose can depend on a number of factors, such as file size/type limitations (e.g., if you are submitting your visualization as part of a conference paper or to a poster printing shop) and where it will be displayed (e.g., online, in a paper, on a poster, on a billboard, in talk slides). Generally speaking, images come in two flavours: bitmap (or raster) formats and vector (or scalable graphics) formats. Bitmap / Raster images are represented as a 2-D grid of square pixels, each with their own colour. Raster images are often compressed before storing so they take up less space. A compressed format is lossy if the image cannot be perfectly recreated when loading and displaying, with the hope that the change is not noticeable. Lossless formats, on the other hand, allow a perfect display of the original image. Common file types: JPEG (.jpg, .jpeg): lossy, usually used for photographs PNG (.png): lossless, usually used for plots / line drawings BMP (.bmp): lossless, raw image data, no compression (rarely used) TIFF (.tif, .tiff): typically lossless, no compression, used mostly in graphic arts, publishing Open-source software: GIMP Vector / Scalable Graphics images are represented as a collection of mathematical objects (lines, surfaces, shapes, curves). When the computer displays the image, it redraws all of the elements using their mathematical formulas. Common file types: SVG (.svg): general-purpose use EPS (.eps), general-purpose use (rarely used) Open-source software: Inkscape Raster and vector images have opposing advantages and disadvantages. A raster image of a fixed width / height takes the same amount of space and time to load regardless of what the image shows (caveat: the compression algorithms may shrink the image more or run faster for certain images). A vector image takes space and time to load corresponding to how complex the image is, since the computer has to draw all the elements each time it is displayed. For example, if you have a scatter plot with 1 million points stored as an SVG file, it may take your computer some time to open the image. On the other hand, you can zoom into / scale up vector graphics as much as you like without the image looking bad, while raster images eventually start to look “pixellated.” PDF files: The portable document format PDF (.pdf) is commonly used to store both raster and vector graphics formats. If you try to open a PDF and it’s taking a long time to load, it may be because there is a complicated vector graphics image that your computer is rendering. Let’s investigate how different image file formats behave with a scatter plot of the Old Faithful data set, which happens to be available in base R under the name faithful: library(svglite) #we need this to save SVG files faithful_plot &lt;- ggplot(data = faithful, aes(x = waiting, y = eruptions))+ geom_point() faithful_plot ggsave(&#39;faithful_plot.png&#39;, faithful_plot) ggsave(&#39;faithful_plot.jpg&#39;, faithful_plot) ggsave(&#39;faithful_plot.bmp&#39;, faithful_plot) ggsave(&#39;faithful_plot.tiff&#39;, faithful_plot) ggsave(&#39;faithful_plot.svg&#39;, faithful_plot) print(paste(&quot;PNG filesize: &quot;, file.info(&#39;faithful_plot.png&#39;)[&#39;size&#39;] / 1000000, &quot;MB&quot;)) ## [1] &quot;PNG filesize: 0.193195 MB&quot; print(paste(&quot;JPG filesize: &quot;, file.info(&#39;faithful_plot.jpg&#39;)[&#39;size&#39;] / 1000000, &quot;MB&quot;)) ## [1] &quot;JPG filesize: 0.203905 MB&quot; print(paste(&quot;BMP filesize: &quot;, file.info(&#39;faithful_plot.bmp&#39;)[&#39;size&#39;] / 1000000, &quot;MB&quot;)) ## [1] &quot;BMP filesize: 12.585658 MB&quot; print(paste(&quot;TIFF filesize: &quot;, file.info(&#39;faithful_plot.tiff&#39;)[&#39;size&#39;] / 1000000, &quot;MB&quot;)) ## [1] &quot;TIFF filesize: 12.588634 MB&quot; print(paste(&quot;SVG filesize: &quot;, file.info(&#39;faithful_plot.svg&#39;)[&#39;size&#39;] / 1000000, &quot;MB&quot;)) ## [1] &quot;SVG filesize: 0.046079 MB&quot; Wow, that’s quite a difference! Notice that for such a simple plot with few graphical elements (points), the vector graphics format (SVG) is over 100 times smaller than the uncompressed raster images (BMP, TIFF). Also note that the JPG format is twice as large as the PNG format, since the JPG compression algorithm is designed for natural images (not plots). Below, we also show what the images look like when we zoom in to a rectangle with only 3 data points. You can see why vector graphics formats are so useful: because they’re just based on mathematical formulas, vector graphics can be scaled up to arbitrary sizes. This makes them great for presentation media of all sizes, from papers to posters to billboards. Zoomed in faithful, raster (PNG, left) and vector (SVG, right) formats. "]
]
